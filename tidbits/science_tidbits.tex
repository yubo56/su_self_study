    \documentclass[12pt]{article}
    \usepackage{fancyhdr, amsmath, amsthm, amssymb, mathtools, lastpage,
    hyperref, enumerate, graphicx, setspace, wasysym, upgreek, listings,
    fouriernc}
    \usepackage[margin=1in]{geometry}
    \usepackage{float}
    \newcommand*{\scinot}[2]{#1\times10^{#2}}
    \newcommand*{\dotp}[2]{\left<#1\,\middle|\,#2\right>}
    \newcommand*{\rd}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
    \newcommand*{\pd}[2]{\frac{\partial#1}{\partial#2}}
    \newcommand*{\rdil}[2]{\mathrm{d}#1 / \mathrm{d}#2}
    \newcommand*{\pdil}[2]{\partial#1 / \partial#2}
    \newcommand*{\rtd}[2]{\frac{\mathrm{d}^2#1}{\mathrm{d}#2^2}}
    \newcommand*{\ptd}[2]{\frac{\partial^2 #1}{\partial#2^2}}
    \newcommand*{\md}[2]{\frac{\mathrm{D}#1}{\mathrm{D}#2}}
    \newcommand*{\pvec}[1]{\vec{#1}^{\,\prime}}
    \newcommand*{\svec}[1]{\vec{#1}\;\!}
    \newcommand*{\bm}[1]{\boldsymbol{\mathbf{#1}}}
    \newcommand*{\uv}[1]{\hat{\bm{#1}}}
    \newcommand*{\ang}[0]{\;\text{\AA}}
    \newcommand*{\mum}[0]{\;\upmu \mathrm{m}}
    \newcommand*{\at}[1]{\left.#1\right|}
    \newcommand*{\bra}[1]{\left<#1\right|}
    \newcommand*{\ket}[1]{\left|#1\right>}
    \newcommand*{\abs}[1]{\left|#1\right|}
    \newcommand*{\ev}[1]{\left\langle#1\right\rangle}
    \newcommand*{\p}[1]{\left(#1\right)}
    \newcommand*{\s}[1]{\left[#1\right]}
    \newcommand*{\z}[1]{\left\{#1\right\}}

    \usepackage[labelfont=bf, font=scriptsize]{caption}\usepackage{tikz}
    \usepackage[font=scriptsize]{subcaption}

    \let\Re\undefined
    \let\Im\undefined
    \DeclareMathOperator{\Res}{Res}
    \DeclareMathOperator{\Re}{Re}
    \DeclareMathOperator{\Im}{Im}
    \DeclareMathOperator{\Log}{Log}
    \DeclareMathOperator{\Arg}{Arg}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\E}{E}
    \DeclareMathOperator{\Var}{Var}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\diag}{diag\;}

    \colorlet{Corr}{red}

    \tikzstyle{circ} % usage: \node[circ, placement] (label) {text};
        = [draw, circle, fill=white, node distance=3cm, minimum height=2em]
    \definecolor{commentgreen}{rgb}{0,0.6,0}
    \lstset{
        basicstyle=\ttfamily\footnotesize,
        frame=single,
        numbers=left,
        showstringspaces=false,
        keywordstyle=\color{blue},
        stringstyle=\color{purple},
        commentstyle=\color{commentgreen},
        morecomment=[l][\color{magenta}]{\#}
    }

\begin{document}

\pagestyle{fancy}
\rhead{Yubo Su --- Tidbits}
\cfoot{\thepage/\pageref{LastPage}}

Separating out research-related tidbits from non-research ones.

\tableofcontents

\section{06/29/19---Collisionless Boltzmann Equation in Galaxies: Landau Damping}

Inspired by \url{https://arxiv.org/pdf/1906.08655.pdf}. The problem is basically
formulated as thus: consider a kinetic-theoretic description of a fluid using
distribution function $f(t, x, p)$ which obeys collisionless Boltzmann equation
$\rd{f}{t} = 0$ (we use $p$ instead of $v$ to work in Hamiltonian coordinates).
Introducing a periodic perturbation to this fluid results in a singular
dispersion relation, which can be resolved via the usual Landau prescription
(consider a perturbation having grown from zero at $t=-\infty$). The dispersion
relation describes \emph{Landau damping} (or growth), in which energy from the
fluid is exchanged with the perturber.

\subsection{Linearized EOM}

The point of the paper is instead to analytically compute the impact of the
perturber on the distribution function, to quantify the \emph{scarring} of a
galaxy upon encounters with a nearby perturber. The equations of motion coupling
the distribution function and gravitational potential are given
\begin{equation}
    \rd{f}{t} = \pd{f}{t} + \z*{f, \mathcal{H}} = 0,
\end{equation}
where $\mathcal{H} = \frac{p^2}{2} + \Phi$ and $\z*{\dots}$ denotes the Poisson
bracket $\z*{f, \mathcal{H}} = \vec{\nabla}_xf \cdot \vec{\nabla}_p \mathcal{H}
- \vec{\nabla}_pf \cdot \vec{\nabla}_x\mathcal{H}$.

If we linearize for perturbation quantities $f_1, \Phi_1$ where $\Phi_1(x)$ does
not depend on the momenta, we obtain
\begin{align*}
    0 &= \pd{f_1}{t} + \z*{f_1, \mathcal{H}_0}
            - \vec{\nabla}_pf \cdot \vec{\nabla}_x \mathcal{H}_0,\\
        &= \pd{f_1}{t} + \vec{\nabla}_x f_1 \cdot \vec{p}
            - \vec{\nabla}_p f_1 \cdot \vec{\nabla}_x \Phi_0
            - \vec{\nabla}_p f_0 \cdot \vec{\nabla}_x \Phi_1.
\end{align*}
Oops welp I guess I never solved this.

\section{02/16/23---Linear Predictive Coding: Autoregressions and Fourier Transforms}

This was a simple enough inquiry initially: given a partial time series that
contains sinusoids, how do we extract the frequency? We know one way to do this
using the FFT, but there are advantages to other techniques. Courtesy of Jeremy
Goodman's pointers.

The trick has to do with autoregressions. Suppose we are looking to extract $l$
frequencies of form $e^{i \omega_m t}$, so that
\begin{equation}
    y_n = \sum\limits_m^l C_m e^{i\omega_m n \Delta t}.
\end{equation}
Thus, if we have at least $2l$ points or so, we should be able to fit for the
$2l$ DOF $C_m$ and $\omega_m$. There can be a noise term above if need be, in
which case more points will smooth out the noise.

What is the trick? Well, we compute the $l$-th order autoregression. In other
words, for each sequence of $l$ points, we can write down the expression
satisfying:
\begin{equation}
    y_n = \sum\limits_{m = 1}^l a_m y_{n - m}.
\end{equation}
With $l$-many such sequences, we have enough equations to solve for the $l$-many
unknowns $a_m$. These can be written in the matrix form:
\begin{equation}
    \begin{bmatrix}
        y_n\\
        y_{n + 1}\\
        \vdots\\
        y_{n + l}
    \end{bmatrix} =
    \begin{bmatrix}
        y_{n - 1} & y_{n - 2} & \dots & y_{n - l}\\
        y_{n} & y_{n - 1} & \dots & y_{n - l + 1}\\
        \vdots & \vdots & \dots & \vdots\\
        y_{n + l - 1} & y_{n + l - 2} & \dots & y_{n - 1}
    \end{bmatrix}
    \begin{bmatrix}
        a_1\\
        a_2\\
        \vdots\\
        a_l
    \end{bmatrix}.\label{eq:autoreg_def}
\end{equation}
These $\z{a_l}$ form the $\mathrm{AR}(l)$ autoregressive model for $y_n$.

This is great, but how do we get the frequencies, or also maybe the growth
rates? Now, we rewrite the above equation as
\begin{equation}
    0 =
    \begin{bmatrix}
        y_{n} & y_{n - 1} & \dots & y_{n - l}\\
        y_{n + 1} & y_{n} & \dots & y_{n - l + 1}\\
        \vdots & \vdots & \dots & \vdots\\
        y_{n + l} & y_{n + l - 1} & \dots & y_{n - 1}
    \end{bmatrix}
    \begin{bmatrix}
        -1\\
        a_1\\
        \vdots\\
        a_l
    \end{bmatrix} \equiv \bm{B} \cdot \vec{a}.
\end{equation}
Now, what if the $y_n$ look like $\lambda^n$ for some complex $\lambda$? Then
the $\lambda$ must satisfy
\begin{equation}
    1 - a_1\lambda - a_2\lambda^2 - \dots - a_l\lambda^l = 0.
\end{equation}
This is the characteristic equation for this $\mathrm{AR}(l)$ model. If we solve
for the roots of this equation, we get the possible values of $\lambda$ that
satisfy the model. In other words, if the $y_n = \lambda^n$ indeed, then $\bm{B}
\cdot \vec{a} = 0$ as requested above. Then, if the data are oscillatory, then
$\lambda = e^{i\omega_m}$ as requested above.

\subsection{Intuitive Understanding}

There's something slightly unintuitive here: we began by seeking the frequencies
in the $y_n$, but we eventually obtained this by solving an equation that has
\emph{nothing} to do with the $y_n$, the characteristic equation for the
$\mathrm{AR}(l)$ model! Why does this make sense?

Well, it should be noted that a particular $\mathrm{AR}(l)$ model does not
uniquely specify the $y_n$; this would be impossible, since there are only $l$
DOF in the model and $2l + 1$ in Eq.~\eqref{eq:autoreg_def}. Indeed, this
suggests that the amplitudes of the modes $C_m$, as well as the initial
normalization of the autoregressive chain ($y_{n - l}$) are free parameters. As
such, we can imagine that the $\mathrm{AR}(l)$ model permits a family of
solutions, any with the correct frequencies. In other words, we could also
imagine writing:
\begin{equation}
    \bm{B} \cdot \ket{a} = \sum\limits_{m=1}^lC_m\ket{b_m}\bra{b_m}\ket{a}.
\end{equation}

Another way to think about the characteristic equation is as exactly a
characteristic equation of a matrix. If we consider the matrix that maps the
vector
\begin{align}
    \begin{bmatrix}
        y_N\\
        y_{N - 1}\\
        \vdots\\
        y_{N - l}
    \end{bmatrix} = \bm{M}
    \begin{bmatrix}
        y_{N - 1}\\
        y_{N - 2}\\
        \vdots\\
        y_{N - l - 1}
    \end{bmatrix},
\end{align}
then it's clear that $\bm{M}$ has the form
\begin{equation}
    \bm{M} = \begin{bmatrix}
        a_1 & a_2 & \dots & a_{l - 1} & a_l\\
        1 & 0 & \dots & 0 & 0\\
        0 & 1 & \dots & 0 & 0\\
        0 & 0 & \dots & 1 & 0
    \end{bmatrix}.
\end{equation}
It's then clear $\bm{M}$ apparently has exactly the characteristic equation that
we prescribe above. This makes sense: the matrix $\bm{M}$ tells us whether a
vector-valued sequence of $y$ values is growing, decaying, or oscillating.

\textbf{As such, the final conclusion of this tidbit is this: the autoregression
is another way of expressing a Markov chain that allows us to advance the time
series. Then, note that any sequence with $y_n = z^n$ where $z$ is complex
(allowing periodic or exponential sequences) has $z$ as one of the eigenvalue of
its Markov chain matrix, or $z$ is a root of its characteristic polynomial.
Turning this on its head, if we compute the autoregression for a sequence and
find a root $w$ of its characteristic polynomial, this implies that the sequence
has a geometric component with factor $w$. Applying this to sequence with a
periodic component with frequency $\omega$, we see that $e^{i\omega \Delta t}$
must be a root of the characteristic polynomial of its autoregression.}

\section{02/21/23---Chaotic vs Diffusive Behavior}

This is a short section. Dong (and others) talk about ``chaotic tides'', where
the mode amplitude grows stochastically because the forcing occurs with random
amplitude. However, this is not chaos, but should be properly termed ``diffusive
tides.''

How can we argue for this? Well, the defining characteristic of chaos is a
positive Lyapunov exponent, i.e.\ an exponential growth rate of the separation
between two trajectories with nearly-idential initial conditions.
\begin{align}
    \delta y(t) &= y(t; y_0) - y(t; y_0 - \epsilon),\\
        &\sim \mathcal{O}(\epsilon e^{\lambda t}).
\end{align}
What is the growth rate for a random walk, or diffusive growth?

Let's adopt the simplest model for now, a discrete random walk with step size
$\pm 1$. Perhaps, for the sake of consistency, we can imagine that the step is
determined based on the current value of $x$, e.g.\ whether the rounded value of
$10^9 x$ is even or odd. Then, consider two initially adjacent $x$. It is
obvious that
\begin{equation}
    \abs{\delta x(t)} \leq 2t.
\end{equation}
So we already see that diffusive behavior is not chaotic.

But now, we have a fun little math problem. Consider two random walks starting
at $x = 0$ with step size $\pm 1$. What is the mean and variance of $\delta x$?
Well, using the usual CLT guidelines, the linearity of expectation gives
$\E\p{X_2 - X_1} = 0$ while linearity of variance gives $\Var\p{X_2 - X_1} =
2\Var(X_1) = 2t$. Thus, the separation between two walkers grows stochastically
and $\sim \sqrt{t}$. This is not chaos, where the separation grows
deterministically and $\sim \exp\p{\lambda t}$.

\end{document}
