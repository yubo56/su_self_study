    \documentclass[12pt]{article}
    \usepackage{fancyhdr, amsmath, amsthm, amssymb, mathtools, lastpage,
    hyperref, enumerate, graphicx, setspace, wasysym, upgreek, listings,
    fouriernc}
    \usepackage[margin=1in]{geometry}
    \usepackage{float}
    \newcommand*{\scinot}[2]{#1\times10^{#2}}
    \newcommand*{\dotp}[2]{\left<#1\,\middle|\,#2\right>}
    \newcommand*{\rd}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
    \newcommand*{\pd}[2]{\frac{\partial#1}{\partial#2}}
    \newcommand*{\rdil}[2]{\mathrm{d}#1 / \mathrm{d}#2}
    \newcommand*{\pdil}[2]{\partial#1 / \partial#2}
    \newcommand*{\rtd}[2]{\frac{\mathrm{d}^2#1}{\mathrm{d}#2^2}}
    \newcommand*{\ptd}[2]{\frac{\partial^2 #1}{\partial#2^2}}
    \newcommand*{\md}[2]{\frac{\mathrm{D}#1}{\mathrm{D}#2}}
    \newcommand*{\pvec}[1]{\vec{#1}^{\,\prime}}
    \newcommand*{\svec}[1]{\vec{#1}\;\!}
    \newcommand*{\bm}[1]{\boldsymbol{\mathbf{#1}}}
    \newcommand*{\uv}[1]{\hat{\bm{#1}}}
    \newcommand*{\ang}[0]{\;\text{\AA}}
    \newcommand*{\mum}[0]{\;\upmu \mathrm{m}}
    \newcommand*{\at}[1]{\left.#1\right|}
    \newcommand*{\bra}[1]{\left<#1\right|}
    \newcommand*{\ket}[1]{\left|#1\right>}
    \newcommand*{\abs}[1]{\left|#1\right|}
    \newcommand*{\ev}[1]{\left\langle#1\right\rangle}
    \newcommand*{\p}[1]{\left(#1\right)}
    \newcommand*{\s}[1]{\left[#1\right]}
    \newcommand*{\z}[1]{\left\{#1\right\}}

    \usepackage[labelfont=bf, font=scriptsize]{caption}\usepackage{tikz}
    \usepackage[font=scriptsize]{subcaption}

    \let\Re\undefined
    \let\Im\undefined
    \DeclareMathOperator{\Res}{Res}
    \DeclareMathOperator{\Re}{Re}
    \DeclareMathOperator{\Im}{Im}
    \DeclareMathOperator{\Log}{Log}
    \DeclareMathOperator{\Arg}{Arg}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\E}{E}
    \DeclareMathOperator{\Var}{Var}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\diag}{diag\;}

    \colorlet{Corr}{red}

    \tikzstyle{circ} % usage: \node[circ, placement] (label) {text};
        = [draw, circle, fill=white, node distance=3cm, minimum height=2em]
    \definecolor{commentgreen}{rgb}{0,0.6,0}
    \lstset{
        basicstyle=\ttfamily\footnotesize,
        frame=single,
        numbers=left,
        showstringspaces=false,
        keywordstyle=\color{blue},
        stringstyle=\color{purple},
        commentstyle=\color{commentgreen},
        morecomment=[l][\color{magenta}]{\#}
    }

\begin{document}

\pagestyle{fancy}
\rhead{Yubo Su --- Tidbits}
\cfoot{\thepage/\pageref{LastPage}}

Separating out research-related tidbits from non-research ones.

\tableofcontents

\section{06/29/19---Collisionless Boltzmann Equation in Galaxies: Landau Damping}

Inspired by \url{https://arxiv.org/pdf/1906.08655.pdf}. The problem is basically
formulated as thus: consider a kinetic-theoretic description of a fluid using
distribution function $f(t, x, p)$ which obeys collisionless Boltzmann equation
$\rd{f}{t} = 0$ (we use $p$ instead of $v$ to work in Hamiltonian coordinates).
Introducing a periodic perturbation to this fluid results in a singular
dispersion relation, which can be resolved via the usual Landau prescription
(consider a perturbation having grown from zero at $t=-\infty$). The dispersion
relation describes \emph{Landau damping} (or growth), in which energy from the
fluid is exchanged with the perturber.

\subsection{Linearized EOM}

The point of the paper is instead to analytically compute the impact of the
perturber on the distribution function, to quantify the \emph{scarring} of a
galaxy upon encounters with a nearby perturber. The equations of motion coupling
the distribution function and gravitational potential are given
\begin{equation}
    \rd{f}{t} = \pd{f}{t} + \z*{f, \mathcal{H}} = 0,
\end{equation}
where $\mathcal{H} = \frac{p^2}{2} + \Phi$ and $\z*{\dots}$ denotes the Poisson
bracket $\z*{f, \mathcal{H}} = \vec{\nabla}_xf \cdot \vec{\nabla}_p \mathcal{H}
- \vec{\nabla}_pf \cdot \vec{\nabla}_x\mathcal{H}$.

If we linearize for perturbation quantities $f_1, \Phi_1$ where $\Phi_1(x)$ does
not depend on the momenta, we obtain
\begin{align*}
    0 &= \pd{f_1}{t} + \z*{f_1, \mathcal{H}_0}
            - \vec{\nabla}_pf \cdot \vec{\nabla}_x \mathcal{H}_0,\\
        &= \pd{f_1}{t} + \vec{\nabla}_x f_1 \cdot \vec{p}
            - \vec{\nabla}_p f_1 \cdot \vec{\nabla}_x \Phi_0
            - \vec{\nabla}_p f_0 \cdot \vec{\nabla}_x \Phi_1.
\end{align*}
Oops welp I guess I never solved this.

\section{02/16/23---Linear Predictive Coding: Autoregressions and Fourier Transforms}

This was a simple enough inquiry initially: given a partial time series that
contains sinusoids, how do we extract the frequency? We know one way to do this
using the FFT, but there are advantages to other techniques. Courtesy of Jeremy
Goodman's pointers.

The trick has to do with autoregressions. Suppose we are looking to extract $l$
frequencies of form $e^{i \omega_m t}$, so that
\begin{equation}
    y_n = \sum\limits_m^l C_m e^{i\omega_m n \Delta t}.
\end{equation}
Thus, if we have at least $2l$ points or so, we should be able to fit for the
$2l$ DOF $C_m$ and $\omega_m$. There can be a noise term above if need be, in
which case more points will smooth out the noise.

What is the trick? Well, we compute the $l$-th order autoregression. In other
words, for each sequence of $l$ points, we can write down the expression
satisfying:
\begin{equation}
    y_n = \sum\limits_{m = 1}^l a_m y_{n - m}.
\end{equation}
With $l$-many such sequences, we have enough equations to solve for the $l$-many
unknowns $a_m$. These can be written in the matrix form:
\begin{equation}
    \begin{bmatrix}
        y_n\\
        y_{n + 1}\\
        \vdots\\
        y_{n + l}
    \end{bmatrix} =
    \begin{bmatrix}
        y_{n - 1} & y_{n - 2} & \dots & y_{n - l}\\
        y_{n} & y_{n - 1} & \dots & y_{n - l + 1}\\
        \vdots & \vdots & \dots & \vdots\\
        y_{n + l - 1} & y_{n + l - 2} & \dots & y_{n - 1}
    \end{bmatrix}
    \begin{bmatrix}
        a_1\\
        a_2\\
        \vdots\\
        a_l
    \end{bmatrix}.\label{eq:autoreg_def}
\end{equation}
These $\z{a_l}$ form the $\mathrm{AR}(l)$ autoregressive model for $y_n$.

This is great, but how do we get the frequencies, or also maybe the growth
rates? Now, we rewrite the above equation as
\begin{equation}
    0 =
    \begin{bmatrix}
        y_{n} & y_{n - 1} & \dots & y_{n - l}\\
        y_{n + 1} & y_{n} & \dots & y_{n - l + 1}\\
        \vdots & \vdots & \dots & \vdots\\
        y_{n + l} & y_{n + l - 1} & \dots & y_{n - 1}
    \end{bmatrix}
    \begin{bmatrix}
        -1\\
        a_1\\
        \vdots\\
        a_l
    \end{bmatrix} \equiv \bm{B} \cdot \vec{a}.
\end{equation}
Now, what if the $y_n$ look like $\lambda^n$ for some complex $\lambda$? Then
the $\lambda$ must satisfy
\begin{equation}
    1 - a_1\lambda - a_2\lambda^2 - \dots - a_l\lambda^l = 0.
\end{equation}
This is the characteristic equation for this $\mathrm{AR}(l)$ model. If we solve
for the roots of this equation, we get the possible values of $\lambda$ that
satisfy the model. In other words, if the $y_n = \lambda^n$ indeed, then $\bm{B}
\cdot \vec{a} = 0$ as requested above. Then, if the data are oscillatory, then
$\lambda = e^{i\omega_m}$ as requested above.

\subsection{Intuitive Understanding}

There's something slightly unintuitive here: we began by seeking the frequencies
in the $y_n$, but we eventually obtained this by solving an equation that has
\emph{nothing} to do with the $y_n$, the characteristic equation for the
$\mathrm{AR}(l)$ model! Why does this make sense?

Well, it should be noted that a particular $\mathrm{AR}(l)$ model does not
uniquely specify the $y_n$; this would be impossible, since there are only $l$
DOF in the model and $2l + 1$ in Eq.~\eqref{eq:autoreg_def}. Indeed, this
suggests that the amplitudes of the modes $C_m$, as well as the initial
normalization of the autoregressive chain ($y_{n - l}$) are free parameters. As
such, we can imagine that the $\mathrm{AR}(l)$ model permits a family of
solutions, any with the correct frequencies. In other words, we could also
imagine writing:
\begin{equation}
    \bm{B} \cdot \ket{a} = \sum\limits_{m=1}^lC_m\ket{b_m}\bra{b_m}\ket{a}.
\end{equation}

Another way to think about the characteristic equation is as exactly a
characteristic equation of a matrix. If we consider the matrix that maps the
vector
\begin{align}
    \begin{bmatrix}
        y_N\\
        y_{N - 1}\\
        \vdots\\
        y_{N - l}
    \end{bmatrix} = \bm{M}
    \begin{bmatrix}
        y_{N - 1}\\
        y_{N - 2}\\
        \vdots\\
        y_{N - l - 1}
    \end{bmatrix},
\end{align}
then it's clear that $\bm{M}$ has the form
\begin{equation}
    \bm{M} = \begin{bmatrix}
        a_1 & a_2 & \dots & a_{l - 1} & a_l\\
        1 & 0 & \dots & 0 & 0\\
        0 & 1 & \dots & 0 & 0\\
        0 & 0 & \dots & 1 & 0
    \end{bmatrix}.
\end{equation}
It's then clear $\bm{M}$ apparently has exactly the characteristic equation that
we prescribe above. This makes sense: the matrix $\bm{M}$ tells us whether a
vector-valued sequence of $y$ values is growing, decaying, or oscillating.

\textbf{As such, the final conclusion of this tidbit is this: the autoregression
is another way of expressing a Markov chain that allows us to advance the time
series. Then, note that any sequence with $y_n = z^n$ where $z$ is complex
(allowing periodic or exponential sequences) has $z$ as one of the eigenvalue of
its Markov chain matrix, or $z$ is a root of its characteristic polynomial.
Turning this on its head, if we compute the autoregression for a sequence and
find a root $w$ of its characteristic polynomial, this implies that the sequence
has a geometric component with factor $w$. Applying this to sequence with a
periodic component with frequency $\omega$, we see that $e^{i\omega \Delta t}$
must be a root of the characteristic polynomial of its autoregression.}

\section{02/21/23---Chaotic vs Diffusive Behavior}

This is a short section. Dong (and others) talk about ``chaotic tides'', where
the mode amplitude grows stochastically because the forcing occurs with random
amplitude. However, this is not chaos, but should be properly termed ``diffusive
tides.''

How can we argue for this? Well, the defining characteristic of chaos is a
positive Lyapunov exponent, i.e.\ an exponential growth rate of the separation
between two trajectories with nearly-idential initial conditions.
\begin{align}
    \delta y(t) &= y(t; y_0) - y(t; y_0 - \epsilon),\\
        &\sim \mathcal{O}(\epsilon e^{\lambda t}).
\end{align}
What is the growth rate for a random walk, or diffusive growth?

Let's adopt the simplest model for now, a discrete random walk with step size
$\pm 1$. Perhaps, for the sake of consistency, we can imagine that the step is
determined based on the current value of $x$, e.g.\ whether the rounded value of
$10^9 x$ is even or odd. Then, consider two initially adjacent $x$. It is
obvious that
\begin{equation}
    \abs{\delta x(t)} \leq 2t.
\end{equation}
So we already see that diffusive behavior is not chaotic.

But now, we have a fun little math problem. Consider two random walks starting
at $x = 0$ with step size $\pm 1$. What is the mean and variance of $\delta x$?
Well, using the usual CLT guidelines, the linearity of expectation gives
$\E\p{X_2 - X_1} = 0$ while linearity of variance gives $\Var\p{X_2 - X_1} =
2\Var(X_1) = 2t$. Thus, the separation between two walkers grows stochastically
and $\sim \sqrt{t}$. This is not chaos, where the separation grows
deterministically and $\sim \exp\p{\lambda t}$.

\section{02/23/23---Rayleigh Distribution}

\subsection{2D}

The Rayleigh distribution is commonly used for mutual inclinations. Can we
briefly give ourselves some intuition for it?

It's known that the Rayleigh distribution is the magnitude of a 2D vector with
two normally-distributed components. Thus, consider if $X$ and $Y$ are drawn
from $N\p{0, \sigma}$. Then the CDF of the magnitude $M = \sqrt{X^2 + Y^2}$ of
the vector is calculated as
\begin{align}
    F_M(m; \sigma) &= \iint\limits_{D(m)}f_U(u; \sigma)
        f_V(v; \sigma)\;\mathrm{d}u\mathrm{d}v
\end{align}
Here, $D(m)$ is the unit disc satisfying $\sqrt{u^2 + v^2} \leq m$, and $f_U =
N(0, \sigma)$ and $f_V = N(0, \sigma)$ are the PDFs of the random variables $U,
V$. Upon changing to polar coordinates and integrating:
\begin{align}
    F_M(m; \sigma) &= \int\limits_0^m 2\pi
        \frac{1}{2\pi \sigma^2}\exp\s{-\frac{u^2 + v^2}{2\sigma^2}}
            \;m'\mathrm{d}m',\\
        &= \frac{1}{\sigma^2}\int\limits_0^m m'\exp\s{-\frac{(m')^2}{2\sigma^2}}
            \;\mathrm{d}m',\\
    f_M(m; \sigma) &= \frac{m}{\sigma^2}\exp\s{-\frac{m^2}{2\sigma^2}}.
\end{align}
This is a Rayleigh distribution with width $\sigma$.

Now, if we know that two vectors have a magnitude separation that is Rayleigh
distributed (like Rayleigh distribution), and we know that they are iid, how can
we obtain their individual distributions (under the right assumptions)? Well,
let's first assume that the four vector components are all normally distributed
with $N\p{0, \sigma}$. Then their separation vector's components are also
normally distributed with:
\begin{align}
    f_{X_1 - X_2}\p{x_1 - x_2; \sigma} &= N\p{0, \sigma\sqrt{2}},\\
    f_{Y_1 - Y_2}\p{y_1 - y_2; \sigma} &= N\p{0, \sigma\sqrt{2}}.
\end{align}
Thus, the separation vector's magnitude is Rayleigh distributed with width
parameter $\sigma \sqrt{2}$. Thus, to generate a pair of vectors whose
separation magnitude is Rayleigh distributed with width $\delta$, we can just
generate the vector components from $N\p{0, \delta / \sqrt{2}}$.

To briefly comment, this obviously doesn't depend on the number of vectors, as
long as the measured Rayleigh distribution is for arbitrary pairs in the system.

\subsection{3D}

What about in 3D\@? This is just as much for my own practice with manipulating
PDFs and CDFs than anything. Consider a vector $\p{v_x, v_y, v_z}$ with all
three components drawn from $N\p{0, \sigma}$. What is the distribution of the
magnitude? Again:
\begin{align}
    F_M(m; \sigma)
        &= \frac{1}{(2\pi \sigma^2)^{3/2}}\iiint_{D(m)}
            f_{Vx}\p{v_x; \sigma}
            f_{Vy}\p{v_y; \sigma}
            f_{Vz}\p{v_z; \sigma}\;\mathrm{d}^3v,\\
        &= \frac{4\pi}{(2\pi \sigma^2)^{3/2}}\int_0^m
            e^{-(m')^2/(2\sigma^2)}\;(m')^2\mathrm{d}m',\\
    f_M(m; \sigma) &= \frac{4\pi m^2}{\p{2\pi \sigma^2}^{3/2}}
            e^{-m^2/(2\sigma^2)}.
\end{align}
This of course can easily generalize: it is clear that in N-D\@:
\begin{equation}
    f^{(N)}_M(m; \sigma)
        = \frac{S^{(N)}_m}{\p{2\pi \sigma^2}^{N / 2}}
            e^{-m^2 / (2\sigma^2)},
\end{equation}
where $S^{(N)}_m$ is the surface area of the $N$-sphere with radius $m$.

And what about the separation vector between some $\vec{v}$ and $\vec{w}$ in
3D\@? Well, each component of $\vec{v} - \vec{w}$ has distribution $N\p{0,
\sigma \sqrt{2}}$ again, so if $U = \abs{\vec{v} - \vec{w}}$, then its PDF is
\begin{align}
    f_U(u; \sigma)
        &= \frac{u^2}{\sigma^3\sqrt{4\pi}}
            e^{-m^2 / (4\sigma^2)}.
\end{align}

\section{03/15/23---Mass Loss and Binary Orbit Change}

Without kicks, Hills 1983 seems to have the best prescription. Time to revisit
this problem now that I've done it wrong literally every time I've tried to do
it.

\subsection{Brute Force Circular}

We start with a circular orbit and in the rest frame of the binary. Call the
pre-ML energy $E$ and post-ML energy $E'$. These are the sums of kinetic and
gravitational potential energies, so
\begin{align}
    E &= \frac{m_1v_1^2}{2} + \frac{m_2v_2^2}{2} - \frac{Gm_1m_2}{a},\\
    E' &= E - \frac{m_1v_1^2}{2}(1 - f) + \frac{Gm_1m_2}{a}\p{1 - f}.
\end{align}
Here, $m_1' = fm_1$ is the post-ML mass. Note then that
\begin{align}
    v_1^2 &= \p{\frac{am_2}{m_{13}}}^2\frac{Gm_{12}}{a}
        = \frac{Gm_2^2}{m_{12}a},\\
    K_{\rm cm} &= \frac{\p{(1 - f)m_1v_1}^2}{2\p{fm_1 + m_2}}.
\end{align}
Here, $K_{\rm cm}$ is the kinetic energy associated with the motion of the
post-ML binary's center of mass. To undergo unbinding, we need $f$ such that $E'
= K_{\rm cm}$, so that in the co-moving frame the binary is unbound. This is a
laborious calculation, but we can write ($f' \equiv 1 - f$ is the fraction of
mass lost from $m_1$)
\begin{align*}
    0 &= E' - K_{\rm cm},\\
        &= -\frac{Gm_1m_2}{2a}
            - \frac{m_1v_1^2}{2}f' + \frac{Gm_1m_2}{a}f'
            - \frac{\p{f'm_1v_1}^2}{2\p{fm_1 + m_2}},\\
        &= -\frac{Gm_1m_2}{2a}
            - \frac{Gm_1m_2^2}{2m_{12}a}f' + \frac{Gm_1m_2}{a}f'
            - \frac{\p{f'm_1}^2}{2\p{fm_1 + m_2}}
                \frac{Gm_2^2}{m_{12}a},\\
        &= -\frac{1}{2} - \frac{m_2}{2m_{12}}f'
            + f'
            - \frac{m_1m_2(f')^2}{2(m_{12} - f'm_1)m_{12}},\\
        &= -(m_{12} - f'm_1)m_{12}
            - m_2\p{m_{12} - f'm_1}f'
            + 2(m_{12} - f'm_1)m_{12}f'
            - m_1m_2(f')^2,\\
        &= -m_{12} + f'(m_1 - m_2 + 2m_{12})
            - 2m_1(f')^2,\\
        &= (f')^2 - \frac{f'}{2}\p{3 + \frac{m_2}{m_1}}
            + \frac{1}{2}\p{1 + \frac{m_2}{m_1}},\\
    f' &= \frac{(3 + q)/2 \pm \sqrt{(3 + q)^2/4 - 2(1+q)}}{2},\\
        &= \frac{(3 + q)/2 \pm ((q-1)/2)}{2},\\
        &= \frac{1 + q}{2},\\
        &= \frac{m_{12}}{2m_1}.
\end{align*}
Whew. This is the canonical result, that we need to lose $f'm_1 =
\frac{m_{12}}{2}$ mass to unbind the system.

\subsection{Easier Circular}

The primary difficulty above was that we had this stupid center of mass kinetic
energy to carry around. This can be simplified if we just recognize that we only
need to compute the contribution of the reduced mass to the energy to understand
whether the system remains bound. Recall that the the kinetic energy of of the
reduced mass component is just
\begin{align}
    K_{\rm red} &= \frac{\mu v_{\rm rel}^2}{2},\\
    E_{\rm red} &= K_{\rm red} - \frac{Gm_{12}\mu}{a} =
            -\frac{Gm_{12}\mu}{2a},\\
    v_{\rm rel}^2 &= \frac{Gm_{12}}{a},\\
    E_{\rm red}' &= \frac{\mu v_{\rm rel}^2}{2}
        - \frac{Gm_{12}'\mu'}{a} = 0,\\
        &= \frac{Gm_{12}\mu'}{2a} - \frac{Gm_{12}'\mu'}{a}.
\end{align}
Thus, we end up with the result that $m_{12}' = m_{12} / 2$ results in a
reduced-mass energy $= 0$ and unbinding. It's important to recognize that
$v_{\rm rel}$ is the relative velocity of the two particles, given by
$\bm{v}_{\rm rel} = \bm{v}_2 - \bm{v}_1$, which does not change with
instantaneous mass loss.

\subsection{Eccentric Unbinding}

The argument in the previous section is much easier to generalize to a general
orbit. Consider that the orbit has semimajor axis $a$ and unbinds when the
separation is at $r$. Then
\begin{align*}
    E_{\rm red} &= \frac{1}{2}\mu v_{\rm rel}^2
        - \frac{Gm_{12}\mu}{r} = -\frac{Gm_{12}\mu}{2a},\\
    \frac{v_{\rm rel}^2}{2} &= Gm_{12}\p{-\frac{1}{2a} + \frac{1}{r}},\\
    E_{\rm red}' &= \frac{1}{2}\mu' v_{\rm rel}^2 - \frac{Gm_{12}'\mu'}{r},\\
        &= \mu'\s{-\frac{Gm_{12}}{2a} + \frac{Gm_{12}}{r}}
            - \frac{Gm_{12}'\mu'}{r}.
\end{align*}
Setting this equal to zero, we find
\begin{align}
    0 &= \mu'\s{-\frac{Gm_{12}}{2a} + \frac{Gm_{12}}{r}}
            - \frac{Gm_{12}'\mu'}{r},\\
    \frac{m_{12}'}{m_{12}} &= -\frac{r}{2a} + 1.
\end{align}
If we re-express $m_{12}' \equiv m_{12} - \Delta$, then we can rewrite
\begin{align}
    1 - \frac{\Delta}{m_{12}} &= 1 - \frac{r}{2a}
        = 1 - \frac{1 - e^2}{2\p{1 + e\cos f}}.
\end{align}
This makes sense: since the mass loss effects a torque on the system, we have to
give it the maximum torque to unbind the system, which occurs at pericenter.
Thus, qualitatively we need a minimum eccentricity of $(1 - e) / 2 \sim m_{12}'
/ m_{12}$ to unbind the system of $m_{12}' > m_{12} / 2$, i.e.\ if the mass loss
is too little.

\subsection{Bound Orbits: Final Eccentricity}

Of course, these exercises can be repeated if we would like for bound orbits,
and tracking the angular momentum of the system as well to get eccentricity.
I may redo this some day, but for now I will just cite the result from Hills
1983, where the final eccentricity is given by
\begin{equation}
    e = \z{1 - (1 - e_0^2)\s{\frac{1 - (2a_0/r)(\Delta / m_{12})}{
        1 - \Delta / m_{12}}^2}}^{1/2}.
\end{equation}

\section{Pendulum Periods}

It might be helpful to just do the simple pendulum in a few ways to get its
period. Specifically, I mean the oscillation of the nondimensionalized EOM
\begin{equation}
    \ddot{\theta} = -\sin\theta.
\end{equation}
In the small angle approximation $\theta \ll 1$, we have that the frequency of
the oscillator is just $1$, so the period is $2\pi$.

\subsection{Lindstedt-Poincar\'e}

I always get this wrong, so let's try again. The zeroth order solution is
$\theta = \epsilon \cos\p{\omega_0 t}$ where $\omega_0 = 1$. Let's next imagine
that the frequency has a small $\epsilon$ dependence, so that
\begin{align*}
    \theta(t) &= \epsilon \cos\p{(1 + \epsilon \omega_1)t},\\
    \ddot{\theta} &= -\epsilon \p{1 + \epsilon \omega_1}^2
            \cos\p{(1 + \epsilon \omega_1)t},\\
            &\approx -\epsilon \cos\p{(1 + \epsilon \omega_1)t}
                + \frac{1}{6}\epsilon^3 \cos^3\p{(1 + \epsilon \omega_1)t}.
\end{align*}
Using the quick identity
\begin{align*}
    \cos^3\theta &= \cos\theta - \cos\theta \sin^2\theta\\
        &= \cos\theta + \frac{\cos\theta\p{\cos 2\theta - 1}}{2}\\
        &= \frac{\cos\theta}{2} + \frac{\cos (3\theta) +
            \sin\theta\sin2\theta}{2}\\
        &= \frac{\cos\theta}{2} + \frac{\cos (3\theta)}{2} +
            \sin^2\theta\cos\theta\\
        &= -\cos^3\theta + \frac{3\cos\theta}{2} + \frac{\cos(3\theta)}{2},\\
    \cos^3\theta &= \frac{3\cos\theta}{4} + \frac{\cos(3\theta)}{4},
\end{align*}
we obtain
\begin{align*}
    -\epsilon \p{1 + \epsilon \omega_1}^2
            \cos\p{(1 + \epsilon \omega_1)t}
            &\approx -\epsilon \cos\p{(1 + \epsilon \omega_1)t}
                + \frac{1}{6}\epsilon^3 \cos^3\p{(1 + \epsilon \omega_1)t}.
\end{align*}
Matching coefficients of the first frequency term, we find
\begin{align*}
    -\epsilon\p{1 + 2\epsilon \omega_1}
        &\approx -\epsilon + \frac{\epsilon^3}{8},\\
    \omega_1 &= -\frac{\epsilon}{16},\\
    \omega &= 1 - \frac{\epsilon^2}{16}.
\end{align*}
In our Ph106b class notes, we solve the Duffing oscillator with this technique,
for which $\ddot{\theta} = -\theta - \epsilon \theta^3$ and we obtain that
$\omega = 1 + (3/8)\epsilon A^2$ for oscillation amplitude $A$. For the simple
pendulum, $\epsilon = -1/6$, and so we recover that $\omega = 1 - A^2/16$. We
didn't do this strictly correctly, I guess, since our small parameter was the
oscillation amplitude $A$ instead of the perturbing term $\epsilon$, but we
obtain the right result: \textbf{the oscillation period grows with larger
amplitude}. We can already anticipate that $\epsilon \to 4$ will produce
problems, and indeed $\epsilon = \pi$ corresponds to the upside-down pendulum.

\subsection{Explicit Integral}

The period of the pendulum can be solved exactly using the method of
quadratures. During oscillation, the total energy of the system is conserved,
\begin{equation}
    E = -\cos \theta + \frac{\dot{\theta}^2}{2}.
\end{equation}
Note that in this notation, $\theta = 0$ is the bottom, so $\theta \in [-\pi,
\pi]$. We would formally derive this by writing down the Lagrangian and making
the Legendre transform, but in the present case if we just identify $p =
\dot{\theta}$ the conjugate momentum to $\theta$ then we find immediately that
$\dot{p} = -\pdil{E}{\theta} = -\sin\theta$ and that $\dot{\theta} = \pdil{E}{p}
= p = \dot{\theta}$. Thus, we can explicitly write:
\begin{align*}
    \dot{\theta}(\theta) &= \sqrt{2(E + \cos \theta)},\\
        &= \sqrt{2\p{\cos\theta - \cos\theta_0}}.
\end{align*}
The period of the pendulum is formally defined such that
\begin{align*}
    P &= 4\int\limits_0^{\theta_0}\rd{t}{\theta}\;\mathrm{d}\theta
\end{align*}
This is the amount of time it takes for the pendulum to go from an initial
condition $\pm \theta_0$ to $0$, so a quarter-period. For sufficiently small
$\theta_0$, the quadrature expression can be expanded
\begin{align*}
    \dot{\theta} &\approx \sqrt{-\theta^2 + \theta_0^2}\\
    P &= 4\int\limits_0^{\theta_0} \frac{1}{\sqrt{\theta_0^2 - \theta^2}}
            \;\mathrm{d}\theta.
\end{align*}
We know this is arcsin, but is it obvious? Actually, yeah:
\begin{align*}
    \int\limits^y\frac{1}{\sqrt{1 - x^2}}\;\mathrm{d}x
        &= \int\limits^{\arcsin(y)}
            \frac{1}{\sqrt{1 - \sin^2u}}\;\mathrm{d}(\sin u),\\
        &= \int\limits^{\arcsin(y)}\mathrm{d}u,\\
        &= \arcsin(y).
\end{align*}
So then
\begin{align*}
    P &= 4\s{\arcsin\p{\frac{\theta}{\theta_0}}}_0^{\theta_0},\\
        &= 2\pi.
\end{align*}
Great.

What about in the nonlinear limit though? In full generality, we have
\begin{align*}
    P &= 4\int\limits_0^{\theta_0}
        \frac{1}{\sqrt{2\p{\cos \theta - \cos \theta_0}}} \;\mathrm{d}\theta.
\end{align*}
In the limit where $\theta_0 \to \pm \pi$, we cannot make any approximations
since $\theta$ will span the full angular interval. However, $\theta_0$
sufficiently close to $\pm \pi$, the unstable points, we recognize that the
dominant contribution to $P$ will be near $\pi$. Thus, let's instead ask the
question: for some fixed $\theta_1 \ll 1$, how long does it take for the
trajectory to reach $\theta_1$ as $\theta_0 \to \pi$? We again should be able to
make expansions now (let $\phi \equiv \pi - \theta$)
\begin{align*}
    P &\gtrsim 4\int\limits_{\phi_0}^{\phi_1}
        \frac{1}{\sqrt{\phi^2 - \phi_0^2}} \;\mathrm{d}\phi.
\end{align*}
Note here that $\phi > \phi_0$. This one is probably a cosh?
\begin{align*}
    \int\limits^y\frac{1}{\sqrt{x^2 - 1}}\;\mathrm{d}x
        &= \int\limits^{\cosh^{-1}(y)}
            \frac{1}{\sqrt{\cosh^2(u) - 1}}\;\mathrm{d}(\cosh(u)),\\
        &= \cosh^{-1}(y),\\
    P &\gtrsim 4\s{\cosh^{-1}\p{\frac{\phi}{\phi_0}}}^{\phi_1}_{\phi_0},\\
        &\gtrsim 4\ln\p{\phi_1 / \phi_0} \sim -4\ln\p{\pi_0}.
\end{align*}
Here, we've made use of the fact that $\cosh(x) \approx \exp(x) / 2$ for large
$x$. Hence, we recover the logarithmic divergence that is expected.

\section{Distributions of Functions of Random Variables}

I can never remember how to do this, so let me just write it down.

If we have a random variable $X$ and a second random variable $Y$ that satisfies
$y = y(x)$, then the PDF of Y is simple:
\begin{align}
    \int\limits f_Y(y)\;\mathrm{d}y
        &= \int\limits f_X(x)\;\mathrm{d}x,\\
    f_Y(y) &= f_X(x)\rd{x}{y}.
\end{align}

What if we have a random variable $Z$ that is a function of two random variables
$X, Y$ satisfying $z(x, y)$? This is a little trickier, but we need to write
down the CDF
\begin{align}
    \int\limits f_Z(z)\;\mathrm{d}z
        &= \iint\limits f_Y(y)f_X(x)\;\mathrm{d}x\mathrm{d}y.
\end{align}

This is no longer solvable in general (but we can often do well in statistical
cases with moment-generating functions, CLT, and others). But for sufficiently
simple dependencies, we can do this. Let's just consider $z = x + y$, for $x, y
\in \mathcal{U}_{[0, 1]}$. This is then easy to do:
\begin{align}
    \int\limits_0^z f_Z(w)\;\mathrm{d}w
        &= \int\limits_0^{\min(z, 1)}
            \int\limits_{0}
                ^{\min(z - y, 1)}\;\mathrm{d}x\;\mathrm{d}y,\\
        &= \int\limits_0^{\min(z, 1)}
            \min\p{z - y, 1}\;\mathrm{d}y,\\
        &=
        \begin{cases}
            \int\limits_0^z
                z - y\;\mathrm{d}y & z < 1\\[10pt]
            \int\limits_0^{z - 1}
                \;\mathrm{d}y +
            \int\limits_{z - 1}^1
                (z - y)\;\mathrm{d}y & z > 1,
        \end{cases}\\
        &=
        \begin{cases}
            z^2 / 2 & z < 1\\
            (z - 1) + (z - 1/2) - z(z-1) + (z - 1)^2/2 & z > 1,
        \end{cases}\\
    f_Z(z) &=
        \begin{cases}
            z & z < 1\\
            2 - z & z > 1.
        \end{cases}
\end{align}
We can do the same for $z = xy$:

\end{document}

