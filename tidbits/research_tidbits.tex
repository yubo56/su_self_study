    \documentclass[12pt]{article}
    \usepackage{fancyhdr, amsmath, amsthm, amssymb, mathtools, lastpage,
    hyperref, enumerate, graphicx, setspace, wasysym, upgreek, listings,
    fouriernc, cancel}
    \usepackage[margin=1in]{geometry}
    \usepackage{float}
    \newcommand*{\scinot}[2]{#1\times10^{#2}}
    \newcommand*{\dotp}[2]{\left<#1\,\middle|\,#2\right>}
    \newcommand*{\rd}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
    \newcommand*{\pd}[2]{\frac{\partial#1}{\partial#2}}
    \newcommand*{\rdil}[2]{\mathrm{d}#1 / \mathrm{d}#2}
    \newcommand*{\pdil}[2]{\partial#1 / \partial#2}
    \newcommand*{\rtd}[2]{\frac{\mathrm{d}^2#1}{\mathrm{d}#2^2}}
    \newcommand*{\ptd}[2]{\frac{\partial^2 #1}{\partial#2^2}}
    \newcommand*{\md}[2]{\frac{\mathrm{D}#1}{\mathrm{D}#2}}
    \newcommand*{\pvec}[1]{\vec{#1}^{\,\prime}}
    \newcommand*{\svec}[1]{\vec{#1}\;\!}
    \newcommand*{\bm}[1]{\boldsymbol{\mathbf{#1}}}
    \newcommand*{\uv}[1]{\hat{\bm{#1}}}
    \newcommand*{\ang}[0]{\;\text{\AA}}
    \newcommand*{\mum}[0]{\;\upmu \mathrm{m}}
    \newcommand*{\at}[1]{\left.#1\right|}
    \newcommand*{\bra}[1]{\left<#1\right|}
    \newcommand*{\ket}[1]{\left|#1\right>}
    \newcommand*{\abs}[1]{\left|#1\right|}
    \newcommand*{\ev}[1]{\left\langle#1\right\rangle}
    \newcommand*{\p}[1]{\left(#1\right)}
    \newcommand*{\s}[1]{\left[#1\right]}
    \newcommand*{\z}[1]{\left\{#1\right\}}

    \usepackage[labelfont=bf, font=scriptsize]{caption}\usepackage{tikz}
    \usepackage[font=scriptsize]{subcaption}

    \let\Re\undefined
    \let\Im\undefined
    \DeclareMathOperator{\Res}{Res}
    \DeclareMathOperator{\Re}{Re}
    \DeclareMathOperator{\Im}{Im}
    \DeclareMathOperator{\Log}{Log}
    \DeclareMathOperator{\Arg}{Arg}
    \DeclareMathOperator{\Tr}{Tr}
    \DeclareMathOperator{\E}{E}
    \DeclareMathOperator{\Var}{Var}
    \DeclareMathOperator*{\argmin}{argmin}
    \DeclareMathOperator*{\argmax}{argmax}
    \DeclareMathOperator{\sgn}{sgn}
    \DeclareMathOperator{\arctanh}{arctanh}
    \DeclareMathOperator{\diag}{diag\;}

    \colorlet{Corr}{red}

    \tikzstyle{circ} % usage: \node[circ, placement] (label) {text};
        = [draw, circle, fill=white, node distance=3cm, minimum height=2em]
    \definecolor{commentgreen}{rgb}{0,0.6,0}
    \lstset{
        basicstyle=\ttfamily\footnotesize,
        frame=single,
        numbers=left,
        showstringspaces=false,
        keywordstyle=\color{blue},
        stringstyle=\color{purple},
        commentstyle=\color{commentgreen},
        morecomment=[l][\color{magenta}]{\#}
    }

\begin{document}

\pagestyle{fancy}
\rhead{Yubo Su --- Tidbits}
\cfoot{\thepage/\pageref{LastPage}}

Separating out research-related tidbits from non-research ones.

\tableofcontents

\section{06/29/19---Collisionless Boltzmann Equation in Galaxies: Landau Damping}

Inspired by \url{https://arxiv.org/pdf/1906.08655.pdf}. The problem is basically
formulated as thus: consider a kinetic-theoretic description of a fluid using
distribution function $f(t, x, p)$ which obeys collisionless Boltzmann equation
$\rd{f}{t} = 0$ (we use $p$ instead of $v$ to work in Hamiltonian coordinates).
Introducing a periodic perturbation to this fluid results in a singular
dispersion relation, which can be resolved via the usual Landau prescription
(consider a perturbation having grown from zero at $t=-\infty$). The dispersion
relation describes \emph{Landau damping} (or growth), in which energy from the
fluid is exchanged with the perturber.

\subsection{Linearized EOM}

The point of the paper is instead to analytically compute the impact of the
perturber on the distribution function, to quantify the \emph{scarring} of a
galaxy upon encounters with a nearby perturber. The equations of motion coupling
the distribution function and gravitational potential are given
\begin{equation}
    \rd{f}{t} = \pd{f}{t} + \z*{f, \mathcal{H}} = 0,
\end{equation}
where $\mathcal{H} = \frac{p^2}{2} + \Phi$ and $\z*{\dots}$ denotes the Poisson
bracket $\z*{f, \mathcal{H}} = \vec{\nabla}_xf \cdot \vec{\nabla}_p \mathcal{H}
- \vec{\nabla}_pf \cdot \vec{\nabla}_x\mathcal{H}$.

If we linearize for perturbation quantities $f_1, \Phi_1$ where $\Phi_1(x)$ does
not depend on the momenta, we obtain
\begin{align*}
    0 &= \pd{f_1}{t} + \z*{f_1, \mathcal{H}_0}
            - \vec{\nabla}_pf \cdot \vec{\nabla}_x \mathcal{H}_0,\\
        &= \pd{f_1}{t} + \vec{\nabla}_x f_1 \cdot \vec{p}
            - \vec{\nabla}_p f_1 \cdot \vec{\nabla}_x \Phi_0
            - \vec{\nabla}_p f_0 \cdot \vec{\nabla}_x \Phi_1.
\end{align*}
Oops welp I guess I never solved this.

\section{02/16/23---Linear Predictive Coding: Autoregressions and Fourier Transforms}

This was a simple enough inquiry initially: given a partial time series that
contains sinusoids, how do we extract the frequency? We know one way to do this
using the FFT, but there are advantages to other techniques. Courtesy of Jeremy
Goodman's pointers.

The trick has to do with autoregressions. Suppose we are looking to extract $l$
frequencies of form $e^{i \omega_m t}$, so that
\begin{equation}
    y_n = \sum\limits_m^l C_m e^{i\omega_m n \Delta t}.
\end{equation}
Thus, if we have at least $2l$ points or so, we should be able to fit for the
$2l$ DOF $C_m$ and $\omega_m$. There can be a noise term above if need be, in
which case more points will smooth out the noise.

What is the trick? Well, we compute the $l$-th order autoregression. In other
words, for each sequence of $l$ points, we can write down the expression
satisfying:
\begin{equation}
    y_n = \sum\limits_{m = 1}^l a_m y_{n - m}.
\end{equation}
With $l$-many such sequences, we have enough equations to solve for the $l$-many
unknowns $a_m$. These can be written in the matrix form:
\begin{equation}
    \begin{bmatrix}
        y_n\\
        y_{n + 1}\\
        \vdots\\
        y_{n + l}
    \end{bmatrix} =
    \begin{bmatrix}
        y_{n - 1} & y_{n - 2} & \dots & y_{n - l}\\
        y_{n} & y_{n - 1} & \dots & y_{n - l + 1}\\
        \vdots & \vdots & \dots & \vdots\\
        y_{n + l - 1} & y_{n + l - 2} & \dots & y_{n - 1}
    \end{bmatrix}
    \begin{bmatrix}
        a_1\\
        a_2\\
        \vdots\\
        a_l
    \end{bmatrix}.\label{eq:autoreg_def}
\end{equation}
These $\z{a_l}$ form the $\mathrm{AR}(l)$ autoregressive model for $y_n$.

This is great, but how do we get the frequencies, or also maybe the growth
rates? Now, we rewrite the above equation as
\begin{equation}
    0 =
    \begin{bmatrix}
        y_{n} & y_{n - 1} & \dots & y_{n - l}\\
        y_{n + 1} & y_{n} & \dots & y_{n - l + 1}\\
        \vdots & \vdots & \dots & \vdots\\
        y_{n + l} & y_{n + l - 1} & \dots & y_{n - 1}
    \end{bmatrix}
    \begin{bmatrix}
        -1\\
        a_1\\
        \vdots\\
        a_l
    \end{bmatrix} \equiv \bm{B} \cdot \vec{a}.
\end{equation}
Now, what if the $y_n$ look like $\lambda^n$ for some complex $\lambda$? Then
the $\lambda$ must satisfy
\begin{equation}
    1 - a_1\lambda - a_2\lambda^2 - \dots - a_l\lambda^l = 0.
\end{equation}
This is the characteristic equation for this $\mathrm{AR}(l)$ model. If we solve
for the roots of this equation, we get the possible values of $\lambda$ that
satisfy the model. In other words, if the $y_n = \lambda^n$ indeed, then $\bm{B}
\cdot \vec{a} = 0$ as requested above. Then, if the data are oscillatory, then
$\lambda = e^{i\omega_m}$ as requested above.

\subsection{Intuitive Understanding}

There's something slightly unintuitive here: we began by seeking the frequencies
in the $y_n$, but we eventually obtained this by solving an equation that has
\emph{nothing} to do with the $y_n$, the characteristic equation for the
$\mathrm{AR}(l)$ model! Why does this make sense?

Well, it should be noted that a particular $\mathrm{AR}(l)$ model does not
uniquely specify the $y_n$; this would be impossible, since there are only $l$
DOF in the model and $2l + 1$ in Eq.~\eqref{eq:autoreg_def}. Indeed, this
suggests that the amplitudes of the modes $C_m$, as well as the initial
normalization of the autoregressive chain ($y_{n - l}$) are free parameters. As
such, we can imagine that the $\mathrm{AR}(l)$ model permits a family of
solutions, any with the correct frequencies. In other words, we could also
imagine writing:
\begin{equation}
    \bm{B} \cdot \ket{a} = \sum\limits_{m=1}^lC_m\ket{b_m}\bra{b_m}\ket{a}.
\end{equation}

Another way to think about the characteristic equation is as exactly a
characteristic equation of a matrix. If we consider the matrix that maps the
vector
\begin{align}
    \begin{bmatrix}
        y_N\\
        y_{N - 1}\\
        \vdots\\
        y_{N - l}
    \end{bmatrix} = \bm{M}
    \begin{bmatrix}
        y_{N - 1}\\
        y_{N - 2}\\
        \vdots\\
        y_{N - l - 1}
    \end{bmatrix},
\end{align}
then it's clear that $\bm{M}$ has the form
\begin{equation}
    \bm{M} = \begin{bmatrix}
        a_1 & a_2 & \dots & a_{l - 1} & a_l\\
        1 & 0 & \dots & 0 & 0\\
        0 & 1 & \dots & 0 & 0\\
        0 & 0 & \dots & 1 & 0
    \end{bmatrix}.
\end{equation}
It's then clear $\bm{M}$ apparently has exactly the characteristic equation that
we prescribe above. This makes sense: the matrix $\bm{M}$ tells us whether a
vector-valued sequence of $y$ values is growing, decaying, or oscillating.

\textbf{As such, the final conclusion of this tidbit is this: the autoregression
is another way of expressing a Markov chain that allows us to advance the time
series. Then, note that any sequence with $y_n = z^n$ where $z$ is complex
(allowing periodic or exponential sequences) has $z$ as one of the eigenvalue of
its Markov chain matrix, or $z$ is a root of its characteristic polynomial.
Turning this on its head, if we compute the autoregression for a sequence and
find a root $w$ of its characteristic polynomial, this implies that the sequence
has a geometric component with factor $w$. Applying this to sequence with a
periodic component with frequency $\omega$, we see that $e^{i\omega \Delta t}$
must be a root of the characteristic polynomial of its autoregression.}

\section{02/21/23---Chaotic vs Diffusive Behavior}

This is a short section. Dong (and others) talk about ``chaotic tides'', where
the mode amplitude grows stochastically because the forcing occurs with random
amplitude. However, this is not chaos, but should be properly termed ``diffusive
tides.''

How can we argue for this? Well, the defining characteristic of chaos is a
positive Lyapunov exponent, i.e.\ an exponential growth rate of the separation
between two trajectories with nearly-idential initial conditions.
\begin{align}
    \delta y(t) &= y(t; y_0) - y(t; y_0 - \epsilon),\\
        &\sim \mathcal{O}(\epsilon e^{\lambda t}).
\end{align}
What is the growth rate for a random walk, or diffusive growth?

Let's adopt the simplest model for now, a discrete random walk with step size
$\pm 1$. Perhaps, for the sake of consistency, we can imagine that the step is
determined based on the current value of $x$, e.g.\ whether the rounded value of
$10^9 x$ is even or odd. Then, consider two initially adjacent $x$. It is
obvious that
\begin{equation}
    \abs{\delta x(t)} \leq 2t.
\end{equation}
So we already see that diffusive behavior is not chaotic.

But now, we have a fun little math problem. Consider two random walks starting
at $x = 0$ with step size $\pm 1$. What is the mean and variance of $\delta x$?
Well, using the usual CLT guidelines, the linearity of expectation gives
$\E\p{X_2 - X_1} = 0$ while linearity of variance gives $\Var\p{X_2 - X_1} =
2\Var(X_1) = 2t$. Thus, the separation between two walkers grows stochastically
and $\sim \sqrt{t}$. This is not chaos, where the separation grows
deterministically and $\sim \exp\p{\lambda t}$.

\section{02/23/23---Rayleigh Distribution}

\subsection{2D}

The Rayleigh distribution is commonly used for mutual inclinations. Can we
briefly give ourselves some intuition for it?

It's known that the Rayleigh distribution is the magnitude of a 2D vector with
two normally-distributed components. Thus, consider if $X$ and $Y$ are drawn
from $N\p{0, \sigma}$. Then the CDF of the magnitude $M = \sqrt{X^2 + Y^2}$ of
the vector is calculated as
\begin{align}
    F_M(m; \sigma) &= \iint\limits_{D(m)}f_U(u; \sigma)
        f_V(v; \sigma)\;\mathrm{d}u\mathrm{d}v
\end{align}
Here, $D(m)$ is the unit disc satisfying $\sqrt{u^2 + v^2} \leq m$, and $f_U =
N(0, \sigma)$ and $f_V = N(0, \sigma)$ are the PDFs of the random variables $U,
V$. Upon changing to polar coordinates and integrating:
\begin{align}
    F_M(m; \sigma) &= \int\limits_0^m 2\pi
        \frac{1}{2\pi \sigma^2}\exp\s{-\frac{u^2 + v^2}{2\sigma^2}}
            \;m'\mathrm{d}m',\\
        &= \frac{1}{\sigma^2}\int\limits_0^m m'\exp\s{-\frac{(m')^2}{2\sigma^2}}
            \;\mathrm{d}m',\\
    f_M(m; \sigma) &= \frac{m}{\sigma^2}\exp\s{-\frac{m^2}{2\sigma^2}}.
\end{align}
This is a Rayleigh distribution with width $\sigma$.

Now, if we know that two vectors have a magnitude separation that is Rayleigh
distributed (like Rayleigh distribution), and we know that they are iid, how can
we obtain their individual distributions (under the right assumptions)? Well,
let's first assume that the four vector components are all normally distributed
with $N\p{0, \sigma}$. Then their separation vector's components are also
normally distributed with:
\begin{align}
    f_{X_1 - X_2}\p{x_1 - x_2; \sigma} &= N\p{0, \sigma\sqrt{2}},\\
    f_{Y_1 - Y_2}\p{y_1 - y_2; \sigma} &= N\p{0, \sigma\sqrt{2}}.
\end{align}
Thus, the separation vector's magnitude is Rayleigh distributed with width
parameter $\sigma \sqrt{2}$. Thus, to generate a pair of vectors whose
separation magnitude is Rayleigh distributed with width $\delta$, we can just
generate the vector components from $N\p{0, \delta / \sqrt{2}}$.

To briefly comment, this obviously doesn't depend on the number of vectors, as
long as the measured Rayleigh distribution is for arbitrary pairs in the system.

\subsection{3D}

What about in 3D\@? This is just as much for my own practice with manipulating
PDFs and CDFs than anything. Consider a vector $\p{v_x, v_y, v_z}$ with all
three components drawn from $N\p{0, \sigma}$. What is the distribution of the
magnitude? Again:
\begin{align}
    F_M(m; \sigma)
        &= \frac{1}{(2\pi \sigma^2)^{3/2}}\iiint_{D(m)}
            f_{Vx}\p{v_x; \sigma}
            f_{Vy}\p{v_y; \sigma}
            f_{Vz}\p{v_z; \sigma}\;\mathrm{d}^3v,\\
        &= \frac{4\pi}{(2\pi \sigma^2)^{3/2}}\int_0^m
            e^{-(m')^2/(2\sigma^2)}\;(m')^2\mathrm{d}m',\\
    f_M(m; \sigma) &= \frac{4\pi m^2}{\p{2\pi \sigma^2}^{3/2}}
            e^{-m^2/(2\sigma^2)}.
\end{align}
This of course can easily generalize: it is clear that in N-D\@:
\begin{equation}
    f^{(N)}_M(m; \sigma)
        = \frac{S^{(N)}_m}{\p{2\pi \sigma^2}^{N / 2}}
            e^{-m^2 / (2\sigma^2)},
\end{equation}
where $S^{(N)}_m$ is the surface area of the $N$-sphere with radius $m$.

And what about the separation vector between some $\vec{v}$ and $\vec{w}$ in
3D\@? Well, each component of $\vec{v} - \vec{w}$ has distribution $N\p{0,
\sigma \sqrt{2}}$ again, so if $U = \abs{\vec{v} - \vec{w}}$, then its PDF is
\begin{align}
    f_U(u; \sigma)
        &= \frac{u^2}{\sigma^3\sqrt{4\pi}}
            e^{-m^2 / (4\sigma^2)}.
\end{align}

\section{03/15/23---Mass Loss and Binary Orbit Change}

Without kicks, Hills 1983 seems to have the best prescription. Time to revisit
this problem now that I've done it wrong literally every time I've tried to do
it.

\subsection{Brute Force Circular}

We start with a circular orbit and in the rest frame of the binary. Call the
pre-ML energy $E$ and post-ML energy $E'$. These are the sums of kinetic and
gravitational potential energies, so
\begin{align}
    E &= \frac{m_1v_1^2}{2} + \frac{m_2v_2^2}{2} - \frac{Gm_1m_2}{a},\\
    E' &= E - \frac{m_1v_1^2}{2}(1 - f) + \frac{Gm_1m_2}{a}\p{1 - f}.
\end{align}
Here, $m_1' = fm_1$ is the post-ML mass. Note then that
\begin{align}
    v_1^2 &= \p{\frac{am_2}{m_{13}}}^2\frac{Gm_{12}}{a}
        = \frac{Gm_2^2}{m_{12}a},\\
    K_{\rm cm} &= \frac{\p{(1 - f)m_1v_1}^2}{2\p{fm_1 + m_2}}.
\end{align}
Here, $K_{\rm cm}$ is the kinetic energy associated with the motion of the
post-ML binary's center of mass. To undergo unbinding, we need $f$ such that $E'
= K_{\rm cm}$, so that in the co-moving frame the binary is unbound. This is a
laborious calculation, but we can write ($f' \equiv 1 - f$ is the fraction of
mass lost from $m_1$)
\begin{align*}
    0 &= E' - K_{\rm cm},\\
        &= -\frac{Gm_1m_2}{2a}
            - \frac{m_1v_1^2}{2}f' + \frac{Gm_1m_2}{a}f'
            - \frac{\p{f'm_1v_1}^2}{2\p{fm_1 + m_2}},\\
        &= -\frac{Gm_1m_2}{2a}
            - \frac{Gm_1m_2^2}{2m_{12}a}f' + \frac{Gm_1m_2}{a}f'
            - \frac{\p{f'm_1}^2}{2\p{fm_1 + m_2}}
                \frac{Gm_2^2}{m_{12}a},\\
        &= -\frac{1}{2} - \frac{m_2}{2m_{12}}f'
            + f'
            - \frac{m_1m_2(f')^2}{2(m_{12} - f'm_1)m_{12}},\\
        &= -(m_{12} - f'm_1)m_{12}
            - m_2\p{m_{12} - f'm_1}f'
            + 2(m_{12} - f'm_1)m_{12}f'
            - m_1m_2(f')^2,\\
        &= -m_{12} + f'(m_1 - m_2 + 2m_{12})
            - 2m_1(f')^2,\\
        &= (f')^2 - \frac{f'}{2}\p{3 + \frac{m_2}{m_1}}
            + \frac{1}{2}\p{1 + \frac{m_2}{m_1}},\\
    f' &= \frac{(3 + q)/2 \pm \sqrt{(3 + q)^2/4 - 2(1+q)}}{2},\\
        &= \frac{(3 + q)/2 \pm ((q-1)/2)}{2},\\
        &= \frac{1 + q}{2},\\
        &= \frac{m_{12}}{2m_1}.
\end{align*}
Whew. This is the canonical result, that we need to lose $f'm_1 =
\frac{m_{12}}{2}$ mass to unbind the system.

\subsection{Easier Circular}

The primary difficulty above was that we had this stupid center of mass kinetic
energy to carry around. This can be simplified if we just recognize that we only
need to compute the contribution of the reduced mass to the energy to understand
whether the system remains bound. Recall that the the kinetic energy of of the
reduced mass component is just
\begin{align}
    K_{\rm red} &= \frac{\mu v_{\rm rel}^2}{2},\\
    E_{\rm red} &= K_{\rm red} - \frac{Gm_{12}\mu}{a} =
            -\frac{Gm_{12}\mu}{2a},\\
    v_{\rm rel}^2 &= \frac{Gm_{12}}{a},\\
    E_{\rm red}' &= \frac{\mu v_{\rm rel}^2}{2}
        - \frac{Gm_{12}'\mu'}{a} = 0,\\
        &= \frac{Gm_{12}\mu'}{2a} - \frac{Gm_{12}'\mu'}{a}.
\end{align}
Thus, we end up with the result that $m_{12}' = m_{12} / 2$ results in a
reduced-mass energy $= 0$ and unbinding. It's important to recognize that
$v_{\rm rel}$ is the relative velocity of the two particles, given by
$\bm{v}_{\rm rel} = \bm{v}_2 - \bm{v}_1$, which does not change with
instantaneous mass loss.

\subsection{Eccentric Unbinding}

The argument in the previous section is much easier to generalize to a general
orbit. Consider that the orbit has semimajor axis $a$ and unbinds when the
separation is at $r$. Then
\begin{align*}
    E_{\rm red} &= \frac{1}{2}\mu v_{\rm rel}^2
        - \frac{Gm_{12}\mu}{r} = -\frac{Gm_{12}\mu}{2a},\\
    \frac{v_{\rm rel}^2}{2} &= Gm_{12}\p{-\frac{1}{2a} + \frac{1}{r}},\\
    E_{\rm red}' &= \frac{1}{2}\mu' v_{\rm rel}^2 - \frac{Gm_{12}'\mu'}{r},\\
        &= \mu'\s{-\frac{Gm_{12}}{2a} + \frac{Gm_{12}}{r}}
            - \frac{Gm_{12}'\mu'}{r}.
\end{align*}
Setting this equal to zero, we find
\begin{align}
    0 &= \mu'\s{-\frac{Gm_{12}}{2a} + \frac{Gm_{12}}{r}}
            - \frac{Gm_{12}'\mu'}{r},\\
    \frac{m_{12}'}{m_{12}} &= -\frac{r}{2a} + 1.
\end{align}
If we re-express $m_{12}' \equiv m_{12} - \Delta$, then we can rewrite
\begin{align}
    1 - \frac{\Delta}{m_{12}} &= 1 - \frac{r}{2a}
        = 1 - \frac{1 - e^2}{2\p{1 + e\cos f}}.
\end{align}
This makes sense: since the mass loss effects a torque on the system, we have to
give it the maximum torque to unbind the system, which occurs at pericenter.
Thus, qualitatively we need a minimum eccentricity of $(1 - e) / 2 \sim m_{12}'
/ m_{12}$ to unbind the system of $m_{12}' > m_{12} / 2$, i.e.\ if the mass loss
is too little.

\subsection{Bound Orbits: Final Eccentricity}

Of course, these exercises can be repeated if we would like for bound orbits,
and tracking the angular momentum of the system as well to get eccentricity.
I may redo this some day, but for now I will just cite the result from Hills
1983, where the final eccentricity is given by
\begin{equation}
    e = \z{1 - (1 - e_0^2)\s{\frac{1 - (2a_0/r)(\Delta / m_{12})}{
        1 - \Delta / m_{12}}^2}}^{1/2}.
\end{equation}

\section{03/23/2023---Pendulum Periods}

It might be helpful to just do the simple pendulum in a few ways to get its
period. Specifically, I mean the oscillation of the nondimensionalized EOM
\begin{equation}
    \ddot{\theta} = -\sin\theta.
\end{equation}
In the small angle approximation $\theta \ll 1$, we have that the frequency of
the oscillator is just $1$, so the period is $2\pi$.

\subsection{Lindstedt-Poincar\'e}

I always get this wrong, so let's try again. The zeroth order solution is
$\theta = \epsilon \cos\p{\omega_0 t}$ where $\omega_0 = 1$. Let's next imagine
that the frequency has a small $\epsilon$ dependence, so that
\begin{align*}
    \theta(t) &= \epsilon \cos\p{(1 + \epsilon \omega_1)t},\\
    \ddot{\theta} &= -\epsilon \p{1 + \epsilon \omega_1}^2
            \cos\p{(1 + \epsilon \omega_1)t},\\
            &\approx -\epsilon \cos\p{(1 + \epsilon \omega_1)t}
                + \frac{1}{6}\epsilon^3 \cos^3\p{(1 + \epsilon \omega_1)t}.
\end{align*}
Using the quick identity
\begin{align*}
    \cos^3\theta &= \cos\theta - \cos\theta \sin^2\theta\\
        &= \cos\theta + \frac{\cos\theta\p{\cos 2\theta - 1}}{2}\\
        &= \frac{\cos\theta}{2} + \frac{\cos (3\theta) +
            \sin\theta\sin2\theta}{2}\\
        &= \frac{\cos\theta}{2} + \frac{\cos (3\theta)}{2} +
            \sin^2\theta\cos\theta\\
        &= -\cos^3\theta + \frac{3\cos\theta}{2} + \frac{\cos(3\theta)}{2},\\
    \cos^3\theta &= \frac{3\cos\theta}{4} + \frac{\cos(3\theta)}{4},
\end{align*}
we obtain
\begin{align*}
    -\epsilon \p{1 + \epsilon \omega_1}^2
            \cos\p{(1 + \epsilon \omega_1)t}
            &\approx -\epsilon \cos\p{(1 + \epsilon \omega_1)t}
                + \frac{1}{6}\epsilon^3 \cos^3\p{(1 + \epsilon \omega_1)t}.
\end{align*}
Matching coefficients of the first frequency term, we find
\begin{align*}
    -\epsilon\p{1 + 2\epsilon \omega_1}
        &\approx -\epsilon + \frac{\epsilon^3}{8},\\
    \omega_1 &= -\frac{\epsilon}{16},\\
    \omega &= 1 - \frac{\epsilon^2}{16}.
\end{align*}
In our Ph106b class notes, we solve the Duffing oscillator with this technique,
for which $\ddot{\theta} = -\theta - \epsilon \theta^3$ and we obtain that
$\omega = 1 + (3/8)\epsilon A^2$ for oscillation amplitude $A$. For the simple
pendulum, $\epsilon = -1/6$, and so we recover that $\omega = 1 - A^2/16$. We
didn't do this strictly correctly, I guess, since our small parameter was the
oscillation amplitude $A$ instead of the perturbing term $\epsilon$, but we
obtain the right result: \textbf{the oscillation period grows with larger
amplitude}. We can already anticipate that $\epsilon \to 4$ will produce
problems, and indeed $\epsilon = \pi$ corresponds to the upside-down pendulum.

\subsection{Explicit Integral}

The period of the pendulum can be solved exactly using the method of
quadratures. During oscillation, the total energy of the system is conserved,
\begin{equation}
    E = -\cos \theta + \frac{\dot{\theta}^2}{2}.
\end{equation}
Note that in this notation, $\theta = 0$ is the bottom, so $\theta \in [-\pi,
\pi]$. We would formally derive this by writing down the Lagrangian and making
the Legendre transform, but in the present case if we just identify $p =
\dot{\theta}$ the conjugate momentum to $\theta$ then we find immediately that
$\dot{p} = -\pdil{E}{\theta} = -\sin\theta$ and that $\dot{\theta} = \pdil{E}{p}
= p = \dot{\theta}$. Thus, we can explicitly write:
\begin{align*}
    \dot{\theta}(\theta) &= \sqrt{2(E + \cos \theta)},\\
        &= \sqrt{2\p{\cos\theta - \cos\theta_0}}.
\end{align*}
The period of the pendulum is formally defined such that
\begin{align*}
    P &= 4\int\limits_0^{\theta_0}\rd{t}{\theta}\;\mathrm{d}\theta
\end{align*}
This is the amount of time it takes for the pendulum to go from an initial
condition $\pm \theta_0$ to $0$, so a quarter-period. For sufficiently small
$\theta_0$, the quadrature expression can be expanded
\begin{align*}
    \dot{\theta} &\approx \sqrt{-\theta^2 + \theta_0^2}\\
    P &= 4\int\limits_0^{\theta_0} \frac{1}{\sqrt{\theta_0^2 - \theta^2}}
            \;\mathrm{d}\theta.
\end{align*}
We know this is arcsin, but is it obvious? Actually, yeah:
\begin{align*}
    \int\limits^y\frac{1}{\sqrt{1 - x^2}}\;\mathrm{d}x
        &= \int\limits^{\arcsin(y)}
            \frac{1}{\sqrt{1 - \sin^2u}}\;\mathrm{d}(\sin u),\\
        &= \int\limits^{\arcsin(y)}\mathrm{d}u,\\
        &= \arcsin(y).
\end{align*}
So then
\begin{align*}
    P &= 4\s{\arcsin\p{\frac{\theta}{\theta_0}}}_0^{\theta_0},\\
        &= 2\pi.
\end{align*}
Great.

What about in the nonlinear limit though? In full generality, we have
\begin{align*}
    P &= 4\int\limits_0^{\theta_0}
        \frac{1}{\sqrt{2\p{\cos \theta - \cos \theta_0}}} \;\mathrm{d}\theta.
\end{align*}
In the limit where $\theta_0 \to \pm \pi$, we cannot make any approximations
since $\theta$ will span the full angular interval. However, $\theta_0$
sufficiently close to $\pm \pi$, the unstable points, we recognize that the
dominant contribution to $P$ will be near $\pi$. Thus, let's instead ask the
question: for some fixed $\theta_1 \ll 1$, how long does it take for the
trajectory to reach $\theta_1$ as $\theta_0 \to \pi$? We again should be able to
make expansions now (let $\phi \equiv \pi - \theta$)
\begin{align*}
    P &\gtrsim 4\int\limits_{\phi_0}^{\phi_1}
        \frac{1}{\sqrt{\phi^2 - \phi_0^2}} \;\mathrm{d}\phi.
\end{align*}
Note here that $\phi > \phi_0$. This one is probably a cosh?
\begin{align*}
    \int\limits^y\frac{1}{\sqrt{x^2 - 1}}\;\mathrm{d}x
        &= \int\limits^{\cosh^{-1}(y)}
            \frac{1}{\sqrt{\cosh^2(u) - 1}}\;\mathrm{d}(\cosh(u)),\\
        &= \cosh^{-1}(y),\\
    P &\gtrsim 4\s{\cosh^{-1}\p{\frac{\phi}{\phi_0}}}^{\phi_1}_{\phi_0},\\
        &\gtrsim 4\ln\p{\phi_1 / \phi_0} \sim -4\ln\p{\pi_0}.
\end{align*}
Here, we've made use of the fact that $\cosh(x) \approx \exp(x) / 2$ for large
$x$. Hence, we recover the logarithmic divergence that is expected.

\section{04/15/2023---Distributions of Functions of Random Variables}

I can never remember how to do this, so let me just write it down.

If we have a random variable $X$ and a second random variable $Y$ that satisfies
$y = y(x)$, then the PDF of Y is simple:
\begin{align}
    \int\limits f_Y(y)\;\mathrm{d}y
        &= \int\limits f_X(x)\;\mathrm{d}x,\\
    f_Y(y) &= f_X(x)\rd{x}{y}.
\end{align}

What if we have a random variable $Z$ that is a function of two random variables
$X, Y$ satisfying $z(x, y)$? This is a little trickier, but we need to write
down the CDF
\begin{align}
    \int\limits f_Z(z)\;\mathrm{d}z
        &= \iint\limits f_Y(y)f_X(x)\;\mathrm{d}x\mathrm{d}y.
\end{align}

This is no longer solvable in general (but we can often do well in statistical
cases with moment-generating functions, CLT, and others). But for sufficiently
simple dependencies, we can do this. Let's just consider $z = x + y$, for $x, y
\in \mathcal{U}_{[0, 1]}$. This is then easy to do:
\begin{align}
    \int\limits_0^z f_Z(w)\;\mathrm{d}w
        &= \int\limits_0^{\min(z, 1)}
            \int\limits_{0}
                ^{\min(z - y, 1)}\;\mathrm{d}x\;\mathrm{d}y,\\
        &= \int\limits_0^{\min(z, 1)}
            \min\p{z - y, 1}\;\mathrm{d}y,\\
        &=
        \begin{cases}
            \int\limits_0^z
                z - y\;\mathrm{d}y & z < 1\\[10pt]
            \int\limits_0^{z - 1}
                \;\mathrm{d}y +
            \int\limits_{z - 1}^1
                (z - y)\;\mathrm{d}y & z > 1,
        \end{cases}\\
        &=
        \begin{cases}
            z^2 / 2 & z < 1\\
            (z - 1) + (z - 1/2) - z(z-1) + (z - 1)^2/2 & z > 1,
        \end{cases}\\
    f_Z(z) &=
        \begin{cases}
            z & z < 1\\
            2 - z & z > 1.
        \end{cases}
\end{align}
We can do the same for $z = xy$:

\section{08/21/2023---Change in Mutual Inclination in Hierarchical Triples due to SNe}

We find that when a hierarchical stellar triple has its inner and outer orbits
initially isotropically distributed, the final mutual inclination distribution
is not isotropic, but is depleted near $90^\circ$. We build a simple
quantitative model analogous to this phenomenon and show that it has an exact
solution.

The essence of this behavior is that: a symmetric SNe in the inner orbit results
in an effective kick to the outer orbit in the plane of the inner orbit, denoted
$\vec{v}_{\rm k, eff}$. Only the component of this kick aligned with the outer
orbit normal contributes to realignment of the outer AM\@. Thus,
\begin{equation}
    \Delta I \lesssim \Delta I_{\max}\propto v_{\rm k, eff}\sin I.
\end{equation}
The actual change in $I$ is approximately symmetrically distributed over the
interval $\s{-\Delta I_{\max}, \Delta I_{\max}}$. Note that, strictly speaking,
neither $I$ nor $\cos I$ are uniformly distributed: imagine that the initial AM
is along $\uv{l}_{\rm b, 0}$, then the final AMs are distributed in an
azimuthally symmetric way about $\uv{l}_{\rm b, 0}$. This is closer to a
symmetric distribution in $I$ than $\cos I$ though.

To see what effect this has on the outer inclination, we imagine a diffusion
equation for $I \in [0, \pi]$ with diffusion coefficient $D_0 \sin I$. This can
be thought of as the cumulative effect of infinitely many, infinitesimally small
effective kicks. This has the form
\begin{equation}
    \pd{f}{t} = \pd{}{I}\s{D_0\sin I\pd{f}{I}},
\end{equation}
where $f(t, I)$ is the PDF of $I$ at time $t$. I really just wanted to solve
this PDE lol.

We first consider the steady-state solutions of this PDE\@. One possible family of
solutions requires that
\begin{align}
    D_0\sin I\pd{f}{I} &= C,\\
    -\sin^2I \pd{f}{\cos I} &= \frac{C}{D_0},\\
    \pd{f}{\cos I} &= \frac{C}{D_0\p{\cos^2 I - 1}}.
\end{align}
But since $\arctanh'(x) = (1 - x^2)^{-1}$, we find that this is just
\begin{equation}
    f(\cos I) = \frac{C}{D_0}\arctanh(\cos I).
\end{equation}
Then $C$ can be set by the normalization of $f(t, I)$. The second homogeneous
solution is simply the family of linear solutions $f(t, I) = A \times I$. If the
IC is symmetric, then the symmetry is preserved under evolution, and we can
require that $f'(t, 0) = 0$ at all times. This thus requires that
\begin{equation}
    f\p{\cos I \in [-1, 1]} = \frac{C}{D_0}\abs{\arctanh(\cos I) - \cos I}.
\end{equation}
This works since $\arctanh'(0) = 1$, so the derivative is indeed zero at $\cos I
= 0$, and the solution is symmetric. $C$ is again set by the normalization,
which I'm too lazy to compute. This is thus the steady-state solution, and we
see that it vanishes at the origin and is singular at $\cos I = \pm 1$. This is
indeed the behavior we were beginning to see!

Of course, with just a single kick, we don't evolve to this steady state $f$,
but only a little bit. Nevertheless, this gives a quantitative model reflecting
the depletion near $\cos I = 0$.

\section{02/21/24---Jeremy and Rotations Generated by Andoyer Momenta}

Jeremy asks a simple question, and as always it's a hard one: the three Andoyer
momenta are $p_g=S$, $p_l=S\cos J$, and $p_h=S \cos i$ (not the usual order).
The first is the total spin AM, and the second two are projections along the
body axis and the orbit axis respectively. The question is: why is $\z{p_l, p_h}
= 0$, the PB, when it's clear that rotations about the body and orbit axes do
not commute?

\subsection{Reminder: Standard AM Poisson Brackets}

This was a rabbit hole, and I don't think I have the right answer, but I first
briefly recall what the PB is. For two functions $f\p{q_i, p_i}$ and $g\p{q_i,
p_i}$, we have that
\begin{equation}
    \z{f, g}
        = \sum\limits_i \pd{f}{q_i}\pd{g}{p_i}
            - \pd{f}{p_i}\pd{g}{q_i}.
\end{equation}
The standard results concern the PBs of the angular momenta, given by $\bm{L} =
\bm{r} \times \bm{p}$, or $L_k = \epsilon_{ijk}r_ip_j$. Then, using the standard
Cartesian $\bm{r}$ and $\bm{p}$ as our canonical coordinates, we can evaluate
the PBs for $\bm{L}$:
\begin{align*}
    \z{L_x, L_y}
        &= \sum\limits_m \pd{L_x}{r_m}\pd{L_y}{p_m}
            - \pd{L_x}{p_m}\pd{L_y}{r_m},\\
        &= \sum\limits_m
            \epsilon_{mjx}p_j \epsilon_{myi}r_i
                - \epsilon_{mxi}r_i \epsilon_{mjy}p_j,\\
        &= \sum\limits_m p_jr_i\p{\delta_{jy}\delta_{xi}
            - \delta_{ji}\delta_{xy}}
            - r_ip_j\p{\delta_{xj}\delta_{iy}
                - \delta_{ij}\delta_{xy}},\\
        &= p_yr_x - r_yp_x = L_z.
\end{align*}
In the third line, we've cyclically permuted indicies and used the usual
relation to simplify $\epsilon_{ijk}\epsilon_{imn}$. Of course, this can be
repeated for the other commutators to obtain that $\z{L_i, L_j} =
\epsilon_{ijk}L_k$.

Now, what about for $L^2$? WLOG
\begin{align*}
    \z{L^2, L_z} &= \sum\limits_i L_i\z{L_i, L_z} + \z{L_i, L_z}L_i,\\
        &= -L_x L_y + L_yL_x - L_yL_x + L_xL_y = 0.
\end{align*}
And of course, similarly for the other two components. What about for a general
power of $L$? Well, it's not hard to show that
\begin{align*}
    \z{L^n, L_z}
        &= \sum\limits_m \sum\limits_i
            nL^{n-1}\rd{L}{L_i}\pd{L_i}{q_i}\pd{L_z}{p_i}
            - nL^{n-1}\rd{L}{L_i}\pd{L_i}{p_i}\pd{L_z}{q_i},\\
        &= nL^{n-1}\sum\limits_i\frac{L_i}{L}\z{L_i, L_z},\\
        &= nL^{n-2}\p{L_xL_y - L_yL_x} = 0.
\end{align*}

\subsection{What does it mean to generate rotation?}

This is the key point that I want to belabor, and hopefully be correct on. When
we speak of $L_z$ generating rotations about the $z$ axis, I think that we mean
that there exists a canonical set of coordinates such that $L_z, \phi_z$ are
canonically conjugate. However, in isolation, it means nothing to argue that a
scalar, which is all that $L_z$ is, \emph{generates} any sort of rotation /
action. It is only when we attach it to a Hamiltonian / symplectic manifold that
we can promote the coordinate to an operator and assign an action to it.

In this sense, for the standard Delaunay variables, $(e, L, L_z)$ and $(M,
\omega, \Omega)$ (we write $e = \sqrt{GMa}$ for simplicity), $L_z$ generates
advancement of $\Omega$ at constant $M, \omega$ (and of course constant $e, L,
L_z$). However, there are plenty of rotations about $\uv{z}$ that do not
preserve these variables (such as a torque on the orbit), and the one that $L_z$
generates here can only be defined in the context of the other variables.

Put another way, suppose my particle is initially at Cartesian coordinates $(0,
-1, 0)$. Then $L_z$ will generate rotation about $z$, which corresponds to
motion in the $+\uv{x}$ direction, but it conserves total AM\@. For comparison, in
the original Cartesian coordinates, $p_x$ generates $x$ velocity at constant
$(z, y, p_z, p_y)$, but it does not conserve AM\@. What is the difference between
these two? At the level of the infinitesimal displacement, nothing; we need the
Hamiltonian structure and the other canonical coordinates to distinguish between
the two.

Thus, for the Andoyer system, consisting of $(l, g, h)$, $(p_l, p_g, p_h)$, it
is clear that $p_l$ and $p_h$ generate motion at fixed $(g, h)$ and fixed $(l,
g)$ respectively. The angles $l$ and $h$ are indeed Euler angles describing the
rotational phase about the orbit and body axes respectively, but the motion
generated by $p_l$ and $p_h$ cannot simply correspond to naive rotation about
these axes, and is instead a constrained rotation.

\textbf{After talking to Jeremy}, he points out that his question is motivated
by ``old quantum mechanics'', where we only knew how to quantize very limited
systems since we didn't have the SE until 1926 (while Planck's quantization was
in 1901 or so). When computing the rotational modes of H$_2$O, there is a third
quantum number due to the triaxiality of the molecule, on top of the usual $(l,
m)$ due to $(L^2, L_z)$. But we can imagine describing its orientation in
Andoyer variables instead; how would the quantization proceed in that case? It
would then appear that the canonical momenta are to be promoted to operators,
and then quantized; how do the operators commute then? We didn't get this far,
but I thought that perhaps one just quantizes the three component rotations;
maybe that's what \url{https://arxiv.org/pdf/2211.11347.pdf} is doing for one of
the Euler angles? Since at least the Andoyer angles are Euler angles, though are
a mixed set of them. Of course Jeremy's questions are hard to answer\dots

\section{Mass Loss Induced Eccentricity}

Consider a particle on a circular orbit with $a_0$ around a central mass
$M_\star$ and initial mass $M$. It loses $\delta m$, ejected behind it with
velocity $v_{\rm ej}$; what is the new sma and eccentricity?

First, we note that the new mass of the particle, $M' = M - \delta m$, is now
moving at velocity $v' = v_0 + \delta v$ where
\begin{align}
    v_0 &= \sqrt{GM_\star / a_0},\\
    \delta v &= \frac{\delta m}{M'}v_{\rm ej}.
\end{align}

Then, computing the new angular momentum of the particle (which now has sma
$a'$), we have (factoring out the particle mass)
\begin{align}
    \sqrt{GM_\star a'(1 - e^2)} &= a_0v',\\
    \frac{GM_\star}{a_0}\frac{a'}{a_0}(1 - e^2) &= (v')^2,\\
    1 - e^2 &= \frac{(v')^2}{v_0^2}\frac{a_0}{a'}.
\end{align}
On the other hand, the energy gives us
\begin{align}
    \frac{(v')^2}{2} - \frac{GM_\star}{a_0} &= -\frac{GM_\star}{2a'},\\
    \frac{(v')^2}{v_0^2} - 2 &= -\frac{a_0}{a'}.
\end{align}
Combining, we find (define $\Delta \equiv \delta v / v_0$)
\begin{align}
    1 - e^2 &= \frac{(v')^2}{v_0^2}\p{2 - \frac{(v')^2}{v_0^2}},\\
        &= \p{1 + \Delta}^2\p{2 - \p{1 + \Delta}^2},\\
        &= 2\p{1 + 2\Delta + \Delta^2}
            - \p{1 + 4\Delta + 6\Delta^2 + 4\Delta^3 + \Delta^4},\\
        &= 1 - 4\Delta^2 - 4\Delta^3 - \Delta^4,\\
    e &\approx 2\Delta = 2\frac{\delta m}{M'}\frac{v_{\rm ej}}{v_0}.
\end{align}

\section{Dissipative Resonance Capture, Canonical Transformations}

\subsection{Canonical Transformations: Review}

Let's just discuss in 1D for now, and review some old results. A set of
variables $(q, p)$ and its Hamiltonian $H(q, p)$ are considered canonical if
they obey Hamilton's equations
\begin{align}
    \rd{q}{t} &= \pd{H}{p}, &
    \rd{p}{t} &= -\pd{H}{q}.
\end{align}
Recall that Hamiltonian evolution conserves phase space volume
\begin{align}
    \frac{1}{\delta V}\rd{\delta V}{t}
        &= \sum\limits_{j=1}^m \pd{F_i}{r_i} = \vec{\nabla} \cdot \bm{F},
\end{align}
where $r_i = \p{p_i\dots p_n, q_i\dots q_n}$, and $\bm{F} = \p{-H_{q_i},\dots,
-H_{q_n}, H_{p_i},\dots H_{p_n}}$ is the flux.

A time-independent transformation is $Q(q, p)$, $P(q, p)$ is canonical if (for
all $H$),
\begin{align}
    K(Q, P) &\equiv H\p{q(Q, P), p(Q, P)},\\
    \rd{Q}{t} &= \pd{K}{P}, &
    \rd{P}{t} &= -\pd{K}{Q}.
\end{align}

The canonicity of two variables is most easily determined by the Poisson
Bracket, which equals $1$ if the transformation is canonical:
\begin{equation}
    \z{Q, P}_{qp}
        \equiv \pd{Q}{q}\pd{P}{p} - \pd{Q}{p}\pd{P}{q} = 1.
\end{equation}
A second definition (Morbidelli 2002, S1.6) is that a transformation is
canonical if there exists a generating function. This is a sufficient, but not
necessary criterion. A third sufficient criterion is the existence of a
Hamiltonian flow (Lie perturbation techniques): if
\begin{align}
    p &= P + \int\limits_0^\epsilon
        \dot{P}\;\mathrm{d}t,\\
    q &= Q + \int\limits_0^\epsilon \dot{Q}\;\mathrm{d}t,\\
    \dot{Q} &= \rd{\chi}{P},\\
    \dot{P} &= -\pd{\chi}{Q},
\end{align}
i.e.\ the time derivatives come from a generating Hamiltonian $\chi\p{Q, P}$,
then the transformation is canonical. This can be interpreted as $(Q, P)$ being
a Hamiltonian flow at time $\epsilon$.

Note conveniently that a function $f(q, p)$ evolves following
\begin{equation}
    \rd{f}{t} = \pd{f}{q}\rd{q}{t} + \pd{f}{p}\rd{p}{t} = \z{f, H}.
\end{equation}
Then, if we expand $f(t)$ about $t=0$, then
\begin{align}
    f(t) &= f(0) + \sum\limits_{i=1}^\infty \frac{t^i}{i!}f^{{n}}(0)\nonumber\\
        &= f(0) + \sum\limits_{i=1}^\infty \frac{t^i}{i!}
            \mathcal{L}_H^{{i}}f(0),\\
    \mathcal{L}_H^{{1}}f &= \z{f, H},\\
    \mathcal{L}_H^{(n)}f &= \mathcal{L}_H^{(1)}\mathcal{L}_H^{(n-1)}f.
\end{align}
We call this the \emph{Lie series of $f$ under the flow of $H$}, and it is
denoted $\mathcal{S}_H^tf$. Then a canonical transformation obeys
\begin{align}
    q &= \mathcal{S}_\chi^\epsilon Q, &
    p &= \mathcal{S}_\chi^\epsilon P.
\end{align}

\subsection{Canonical Transformations: Why does it work?}

Landau and Lifshitz have a very slick argument based on the variational
principle (which actually recovers the extra $\partial S/\partial t$ change to
the new Hamiltonian), but I need to convince myself in a dumber way.

% Let's just explicitly evaluate the conditions:
% \begin{align}
%     \rd{Q(q, p)}{t} &= \pd{Q}{q}\pd{H}{p} - \pd{Q}{p}\pd{H}{q},\\
%     \rd{P(q, p)}{t} &= \pd{P}{q}\pd{H}{p} - \pd{P}{p}\pd{H}{q},\\
%     \pd{K}{Q} = \pd{}{Q}H\p{q(Q, P), p(Q, P)}
%         &= \pd{H}{q}\pd{q}{Q} + \pd{H}{p}\pd{p}{Q} = -\rd{P}{t},\\
%     \pd{K}{P} &= \pd{H}{q}\pd{q}{P} + \pd{H}{p}\pd{p}{P} = \rd{Q}{t},\\
%     \pd{Q}{q} &= \pd{p}{P},\\
%     \pd{Q}{p} &= -\pd{q}{P}.
% \end{align}
% Doing this with either set of equations gives the same result. But this doesn't
% make sense, since for a general canonical transformation this produces that
% $\z{Q, P}_{qp} = 2$?

Apparently the easier way is to show that the PB WRT $(Q, P)$ and WRT $(q, p)$
is the same iff $\z{Q, P}_{qp} = 1$, which then trivially results in our desired
claim. Is this true?
% https://astro.pas.rochester.edu/~aquillen/phy411/lecture2.pdf
\begin{align}
    \z{f, g}_{qp} &= \pd{f}{q}\pd{g}{p} - \pd{f}{p}\pd{g}{q},\\
        &= \s{\pd{f}{Q}\pd{Q}{q}
            + \pd{f}{P}\pd{P}{q}}
            \s{\pd{g}{Q}\pd{Q}{p}
            + \pd{g}{P}\pd{P}{p}}
            - \s{\pd{f}{Q}\pd{Q}{p}
            + \pd{f}{P}\pd{P}{p}}
            \s{\pd{g}{Q}\pd{Q}{q}
            + \pd{g}{P}\pd{P}{q}},\\
        &= \cancel{f_Q Q_q g_Q Q_p} + f_QQ_q g_PP_p
            + f_PP_qg_QQ_p + \cancel{f_PP_qg_PP_p}\nonumber\\
        & - \cancel{f_QQ_pg_QQ_q} - f_QQ_pg_PP_q
            - f_PP_pg_QQ_q - \cancel{f_PP_pg_PP_q},\\
        &= \s{\pd{f}{Q}\pd{g}{P} - \pd{f}{P}\pd{g}{Q}}\z{Q, P}_{qp}
            = \z{f, g}_{QP}\z{Q, P}_{qp}
\end{align}
It's a relatively long expansion with chain rules, but we get the desired
result. Then, directly evaluating, we trivially obtain.
\begin{align}
    \rd{Q}{t} &= \z{Q, H}_{qp} = \z{Q, H\p{q(Q, P), p(Q, p)}}_{QP}
        = \pd{K}{P},\\
    \rd{P}{t} &= -\pd{K}{Q}.
\end{align}
With a time dependence,
\begin{align}
    \rd{Q}{t} &= \pd{K}{P} + \pd{Q}{t},\\
        &= \pd{}{P}\p{K + \pd{S}{t}},\\
    \rd{P}{t} &= -\pd{}{Q}\p{K + \pd{S}{t}}.
\end{align}
$S$ is the generating function, and can have various arguments, but is always
added to $K$ and must have derivatives $\dot{S}_P = \pdil{Q}{t}$ and $\dot{S}_Q
= -\pdil{P}{t}$.

\subsection{Henrard Result}

We work backwards, so that we can start with a Hamiltonian system. Consider the
set of variables satisfying
\begin{align}
    \rd{Q}{\tau} &= \pd{}{P}K\p{Q, P}, &
    \rd{P}{\tau} &= -\pd{}{Q}K\p{Q, P}.
\end{align}
Then, the time evolution of some $q\p{Q, P, \tau}$ and $p\p{Q, P, \tau}$ follow
\begin{align}
    \rd{q}{\tau} &= \z{q, K}_{QP} + \pd{q}{\tau},\\
        &= \z{q, K}_{qp}\z{q, p}_{QP} + \pd{q}{\tau},\\
        &= \pd{K}{p}\z{q, p}_{QP} + \pd{q}{\tau},\\
    \rd{q}{t} &= \pd{K}{p} + \pd{q}{t},\\
    \mathrm{d}t &\equiv \mathrm{d}\tau \z{q, p}_{QP},\\
    \rd{p}{t} &= -\pd{K}{q} + \pd{p}{t}.
\end{align}
But if we define $H\p{q, p, t} \equiv K\p{Q(q, p, t), P(q, p, t)}$, then
\begin{align}
    \rd{q}{t} &= \pd{H}{p} + \pd{q}{t},\\
    \rd{p}{t} &= -\pd{H}{q} + \pd{p}{t}.
\end{align}
Thus, if the goal is to match some near-Hamiltonian system with EOM
\begin{align}
    \rd{q}{t} &= \pd{H}{p} + \epsilon f(q, p),\\
    \rd{p}{t} &= -\pd{H}{q} + \epsilon g(q, p),
\end{align}
then we must have
\begin{align}
    q(Q, P, t) &= q_0(Q, P) + \int\limits_0^t
        \epsilon f(q, p)\;\mathrm{d}t',\\
    p(Q, P, t) &= p_0(Q, P) + \int\limits_0^t
        \epsilon g(q, p)\;\mathrm{d}t'.
\end{align}
Recall that we have freedom on the $Q, P$ that we're trying to choose to find
the canonical transformation, so we obviously choose $q_0 = Q$ and $p_0 = P$, a
\emph{near-identity, non-canonical transformation} between $(q, p)$ and $(Q,
P)$. We thus land on
\begin{align}
    q &= Q + \int\limits_0^t \epsilon f(q, p)\;\mathrm{d}t',\\
    p &= P + \int\limits_0^t \epsilon g(q, p)\;\mathrm{d}t'.
\end{align}
In Henrard's notation, these $(q, p)$ are denoted by $(\tilde{q}, \tilde{p})$.
And the interpretation is just that, if we let $K(Q, P) = H(\tilde{q},
\tilde{p})$, then the resulting equations of motion for $\dot{q}$ and $\dot{p}$
will be as desired? A little unsatisfying, to be honest, but I'm done here.

\subsection{Forward Direction}

Okay, fine, I'm not done, let's prove this in the forward direction. Let $q, p$
satisfy the equations of motion
\begin{align}
    \rd{q}{t} &= f_0(q, p) + \epsilon f_1(q, p),\\
    \rd{p}{t} &= g_0(q, p) + \epsilon g_1(q, p).
\end{align}
Here, $f_0(q, p) = \pdil{H}{p}$ and $g_0(q, p) = -\pdil{H}{q}$.

Define the variables $Q(q, p, t), P(q, p, t)$ that satisfy
\begin{align}
    Q &= q - \int\limits_0^t \epsilon f_1(q, p)\;\mathrm{d}t',\\
    P &= p - \int\limits_0^t \epsilon g_1(q, p)\;\mathrm{d}t'.
\end{align}
Inverting these relationships, we define a function of $Q, P$ via
\begin{equation}
    K(Q, P) \equiv H(q(Q, P, t), p(Q, P, t)).
\end{equation}
It's then straightforward to show that
\begin{align}
    \rd{Q}{t} &= \rd{q}{t} - \epsilon f_1(q, p),\\
    \pd{K}{P} &= \pd{H}{p}\pd{p}{P} = f_0(q, p),\\
    \rd{P}{t} &= \rd{p}{t} - \epsilon g_1(q, p),\\
    -\pd{K}{Q} &= -\pd{H}{q}\pd{q}{Q} = g_0(q, p).
\end{align}
It is then clear that, by construction, $Q, P$ and $K(Q, P)$ satisfy Hamilton's
Equations, but I'm missing a factor of the PB\@. So this construction is still
misinterpreting Henrard's idea somewhat.

\section{Tides with Fluid Envelope}

Question: for a planet w/ thin fluid envelope (by mass), it affects the
dissipation of the rocky core, since the tidal bulge of the envelope will shield
the perturbing potential somewhat. Can we estimate this effect? Moreover, how
much does this effect depend on the radial extent of the envelope; can we check
the effect of tidal inflation on the dissipation of a SN that is a rocky core +
gaseous envelope?

\end{document}

